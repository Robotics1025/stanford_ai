{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ac490c-66c4-44ad-b1cd-501a7b62ac9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# AI Programming Assignments\n",
    "\n",
    "## Group C\n",
    "\n",
    "### Team Members\n",
    "\n",
    "| No | Name              | Student Number | Reg. No         |\n",
    "|----|------------------|----------------|----------------|\n",
    "| 1  | Kigozi Allan      | 2400725792     | 24/U/25792/PS  |\n",
    "| 2  | Keith Paul Kato   | 2400726593     | 24/U/26593/EVE |\n",
    "| 3  | Mugole Joel       | 2400707060     | 24/U/07060/EVE |\n",
    "| 4  | Nalubega Shadiah  | 2400708715     | 24/U/08715/EVE |\n",
    "| 5  | Ageno Elizabeth   | 2400725850     | 24/U/25850/PS  |\n",
    "\n",
    "---\n",
    "\n",
    "**Course:** Artificial Intelligence  \n",
    "**Instructor:** Dr. Nakibule Mary  \n",
    "**Date:** October 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7703dd-c536-441c-a370-86deb6d0e737",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 1\n",
    "## Title: Foundations \n",
    "\n",
    "**Description:**  \n",
    "This notebook contains our group’s implementation for the *Foundations* assignment.  \n",
    "\n",
    "The tasks focus on fundamental programming principles, including:  \n",
    "- String manipulation  \n",
    "- Vector operations  \n",
    "- Recursion  \n",
    "- Set-based reasoning  \n",
    "\n",
    "Each function is implemented with clarity and efficiency, demonstrating our understanding of:  \n",
    "- Python’s data structures  \n",
    "- Algorithmic design  \n",
    "\n",
    "**Note:** All code is documented for readability and maintainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298839c6-0f60-4866-8df0-4428314e8986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#--------importing modules for foundations---------------\n",
    "import collections\n",
    "import math\n",
    "from typing import Any, DefaultDict, List, Set, Tuple\n",
    "\n",
    "\n",
    "# --------Type aliases for readability-------\n",
    "SparseVector = DefaultDict[Any, float]\n",
    "Position = Tuple[int, int]\n",
    "\n",
    "# ===========================================\n",
    "# ### Function 1: find_alphabetically_first_word\n",
    "# This function identifies the lexicographically smallest word in a given text.\n",
    "# The comparison is case-sensitive and whitespace is used as the delimiter.\n",
    "# ===========================================\n",
    "\n",
    "def find_alphabetically_first_word(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a string |text|, return the word in |text| that comes first\n",
    "    lexicographically. Words are split by whitespace. Case-sensitive.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    return min(words)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ### Function 2: euclidean_distance\n",
    "# Computes the standard Euclidean distance between two 2D points.\n",
    "# This function applies the Pythagorean theorem and returns a float value.\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def euclidean_distance(loc1: Position, loc2: Position) -> float:\n",
    "    \"\"\"\n",
    "    Return the Euclidean distance between two 2D positions.\n",
    "    \"\"\"\n",
    "    return math.sqrt((loc1[0] - loc2[0]) ** 2 + (loc1[1] - loc2[1]) ** 2)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ### Function 3: mutate_sentences\n",
    "# This function generates all possible sentences of the same length such that\n",
    "# every adjacent pair of words appears in the original sentence in the same order.\n",
    "# The solution uses a depth-first search (DFS) to explore all valid combinations.\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def mutate_sentences(sentence: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate all sentences of the same length where each adjacent pair\n",
    "    appears in the original sentence in the same order.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    # Build adjacency list: each word maps to words that can follow it\n",
    "    adj = collections.defaultdict(set)\n",
    "    for a, b in zip(words[:-1], words[1:]):\n",
    "        adj[a].add(b)\n",
    "\n",
    "    n = len(words)\n",
    "    results = set()\n",
    "\n",
    "    # Depth-first search to construct valid sentences\n",
    "    def dfs(path):\n",
    "        if len(path) == n:\n",
    "            results.add(\" \".join(path))\n",
    "            return\n",
    "        last = path[-1]\n",
    "        for nxt in adj.get(last, []):\n",
    "            dfs(path + [nxt])\n",
    "\n",
    "    for start in words:\n",
    "        dfs([start])\n",
    "\n",
    "    return list(results)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ### Function 4: sparse_vector_dot_product\n",
    "# Computes the dot product between two sparse vectors represented as dictionaries.\n",
    "# This approach ensures efficiency by only iterating through the nonzero keys of v1.\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def sparse_vector_dot_product(v1: SparseVector, v2: SparseVector) -> float:\n",
    "    \"\"\"\n",
    "    Dot product of two sparse vectors represented as defaultdicts.\n",
    "    \"\"\"\n",
    "    return sum(v1[k] * v2[k] for k in v1 if k in v2)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ### Function 5: increment_sparse_vector\n",
    "# Performs an in-place update of one sparse vector by scaling and adding another.\n",
    "# This is a fundamental operation in gradient-based learning algorithms.\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def increment_sparse_vector(v1: SparseVector, scale: float, v2: SparseVector) -> None:\n",
    "    \"\"\"\n",
    "    Perform v1 += scale * v2 (in-place). v2 must not be modified.\n",
    "    \"\"\"\n",
    "    for k, val in v2.items():\n",
    "        v1[k] += scale * val\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ### Function 6: find_nonsingleton_words\n",
    "# Identifies all words that occur more than once in a text.\n",
    "# The function returns a set of such words, making it useful for frequency analysis.\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def find_nonsingleton_words(text: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Return the set of words that occur more than once in text (split on whitespace).\n",
    "    \"\"\"\n",
    "    counts = collections.defaultdict(int)\n",
    "    for w in text.split():\n",
    "        counts[w] += 1\n",
    "    return {w for w, c in counts.items() if c > 1}\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ### MAIN BLOCK - Comprehensive Testing and Demonstrations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 1: FOUNDATIONS - COMPREHENSIVE DEMOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Demo 1: find_alphabetically_first_word\n",
    "    print(\"\\n--- Demo 1: Find Alphabetically First Word ---\")\n",
    "    test_texts = [\n",
    "        \"zebra apple ball\",\n",
    "        \"The quick brown fox\",\n",
    "        \"Python Java C++ Ruby\",\n",
    "        \"artificial intelligence machine learning\"\n",
    "    ]\n",
    "    for text in test_texts:\n",
    "        result = find_alphabetically_first_word(text)\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"First word: '{result}'\\n\")\n",
    "    \n",
    "    # Demo 2: euclidean_distance\n",
    "    print(\"\\n--- Demo 2: Euclidean Distance ---\")\n",
    "    test_positions = [\n",
    "        ((0, 0), (3, 4)),\n",
    "        ((1, 1), (4, 5)),\n",
    "        ((0, 0), (0, 0)),\n",
    "        ((-3, -4), (3, 4))\n",
    "    ]\n",
    "    for pos1, pos2 in test_positions:\n",
    "        dist = euclidean_distance(pos1, pos2)\n",
    "        print(f\"Distance from {pos1} to {pos2}: {dist:.4f}\")\n",
    "    \n",
    "    # Demo 3: mutate_sentences\n",
    "    print(\"\\n--- Demo 3: Mutate Sentences ---\")\n",
    "    test_sentences = [\n",
    "        \"the cat sat\",\n",
    "        \"a b c\",\n",
    "        \"I am what I am\"\n",
    "    ]\n",
    "    for sent in test_sentences:\n",
    "        mutations = mutate_sentences(sent)\n",
    "        print(f\"Original: '{sent}'\")\n",
    "        print(f\"Mutations ({len(mutations)} total):\")\n",
    "        for m in sorted(mutations):\n",
    "            print(f\"  - {m}\")\n",
    "        print()\n",
    "    \n",
    "    # Demo 4: sparse_vector_dot_product\n",
    "    print(\"\\n--- Demo 4: Sparse Vector Dot Product ---\")\n",
    "    v1 = collections.defaultdict(float, {\"a\": 1.0, \"b\": 2.0, \"c\": 3.0})\n",
    "    v2 = collections.defaultdict(float, {\"b\": 0.75, \"c\": 2.0, \"d\": 1.0})\n",
    "    v3 = collections.defaultdict(float, {\"x\": 5.0, \"y\": 10.0})\n",
    "    \n",
    "    print(f\"v1 = {dict(v1)}\")\n",
    "    print(f\"v2 = {dict(v2)}\")\n",
    "    print(f\"v3 = {dict(v3)}\")\n",
    "    print(f\"v1 · v2 = {sparse_vector_dot_product(v1, v2)}\")\n",
    "    print(f\"v1 · v3 = {sparse_vector_dot_product(v1, v3)}\")\n",
    "    print(f\"v2 · v2 = {sparse_vector_dot_product(v2, v2)}\")\n",
    "    \n",
    "    # Demo 5: increment_sparse_vector\n",
    "    print(\"\\n--- Demo 5: Increment Sparse Vector ---\")\n",
    "    v1_original = collections.defaultdict(float, {\"a\": 1.0, \"b\": 2.0, \"c\": 3.0})\n",
    "    v2_scale = collections.defaultdict(float, {\"b\": 0.75, \"c\": 2.0, \"d\": 1.0})\n",
    "    \n",
    "    v1_copy = collections.defaultdict(float, v1_original)\n",
    "    print(f\"Before: v1 = {dict(v1_copy)}\")\n",
    "    print(f\"Adding: 0.5 * v2 where v2 = {dict(v2_scale)}\")\n",
    "    increment_sparse_vector(v1_copy, 0.5, v2_scale)\n",
    "    print(f\"After:  v1 = {dict(v1_copy)}\")\n",
    "    \n",
    "    v1_copy2 = collections.defaultdict(float, v1_original)\n",
    "    print(f\"\\nBefore: v1 = {dict(v1_copy2)}\")\n",
    "    print(f\"Adding: -1.0 * v2\")\n",
    "    increment_sparse_vector(v1_copy2, -1.0, v2_scale)\n",
    "    print(f\"After:  v1 = {dict(v1_copy2)}\")\n",
    "    \n",
    "    # Demo 6: find_nonsingleton_words\n",
    "    print(\"\\n--- Demo 6: Find Non-Singleton Words ---\")\n",
    "    test_word_texts = [\n",
    "        \"a b a c b d a\",\n",
    "        \"hello world hello universe\",\n",
    "        \"the quick brown fox jumps over the lazy dog\",\n",
    "        \"Python is great Python is powerful\"\n",
    "    ]\n",
    "    for text in test_word_texts:\n",
    "        repeated = find_nonsingleton_words(text)\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Repeated words: {sorted(repeated)}\\n\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 1: ALL DEMOS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a642cea-b8db-4a29-9352-6f6db75190db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 2  \n",
    "## Title: Sentiment Classification\n",
    "**Description:**  \n",
    "This assignment implements various components of a sentiment classification system, including:  \n",
    "- Feature extraction  \n",
    "- Stochastic gradient descent learning  \n",
    "- Character n-gram models  \n",
    "- K-means clustering  \n",
    "\n",
    "Each section is explained in detail with formal academic documentation, ensuring clarity and reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f05cf30-0b23-4fa6-94ee-af3bf857ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ASSIGNMENT 2: SENTIMENT CLASSIFICATION - COMPREHENSIVE DEMOS\n",
      "======================================================================\n",
      "\n",
      "--- Demo 1: Word Feature Extraction ---\n",
      "Text: 'I am what I am'\n",
      "Features: {'I': 2, 'am': 2, 'what': 1}\n",
      "\n",
      "Text: 'good movie great film'\n",
      "Features: {'good': 1, 'movie': 1, 'great': 1, 'film': 1}\n",
      "\n",
      "Text: 'Python programming is awesome'\n",
      "Features: {'Python': 1, 'programming': 1, 'is': 1, 'awesome': 1}\n",
      "\n",
      "\n",
      "--- Demo 2: Sentiment Classification with SGD ---\n",
      "Training simple sentiment classifier...\n",
      "Training on 6 examples...\n",
      "Epoch 1: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 2: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 3: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 4: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 5: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 6: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 7: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 8: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 9: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 10: train error = 0.0000, validation error = 0.5000\n",
      "\n",
      "Learned weights (top 10): {'excellent': 0.9999999999999999, 'great': 0.9999999999999999, 'wonderful': 0.9999999999999999, 'bad': -0.9999999999999999, 'terrible': -0.9999999999999999, 'awful': -0.9999999999999999, 'movie': 0.0, 'film': 0.0, 'show': 0.0}\n",
      "\n",
      "--- Testing Predictions ---\n",
      "'great performance' -> score: 1.0000, prediction: positive\n",
      "'bad acting' -> score: -1.0000, prediction: negative\n",
      "'excellent show' -> score: 1.0000, prediction: positive\n",
      "\n",
      "--- Demo 3: Synthetic Dataset Generation ---\n",
      "True weights: {'feature1': 2.0, 'feature2': -1.5, 'feature3': 0.5}\n",
      "Generated 20 examples:\n",
      "  Example 1: features={'feature1': 0, 'feature2': 0, 'feature3': 2}, label=1\n",
      "  Example 2: features={'feature1': 1, 'feature2': 1, 'feature3': 1}, label=1\n",
      "  Example 3: features={'feature1': 0, 'feature2': 0, 'feature3': 3}, label=1\n",
      "  Example 4: features={'feature1': 0, 'feature2': 0, 'feature3': 0}, label=1\n",
      "  Example 5: features={'feature1': 1, 'feature2': 1, 'feature3': 0}, label=1\n",
      "  ... (showing first 5 of 20)\n",
      "\n",
      "--- Demo 4: Character N-Gram Features ---\n",
      "n=2, text='hello world', 2-grams: {'he': 1, 'el': 1, 'll': 1, 'lo': 1, 'ow': 1, 'wo': 1, 'or': 1, 'rl': 1, 'ld': 1}\n",
      "n=3, text='hello world', 3-grams: {'hel': 1, 'ell': 1, 'llo': 1, 'low': 1, 'owo': 1, 'wor': 1, 'orl': 1, 'rld': 1}\n",
      "n=4, text='hello world', 4-grams: {'hell': 1, 'ello': 1, 'llow': 1, 'lowo': 1, 'owor': 1, 'worl': 1, 'orld': 1}\n",
      "\n",
      "--- Demo 5: Learning with Character N-Grams ---\n",
      "\n",
      "Testing extractCharacterFeatures with n=2\n",
      "Epoch 1: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 2: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 3: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 4: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 5: train error = 0.0000, validation error = 0.5000\n",
      "Testing completed.\n",
      "\n",
      "\n",
      "Testing extractCharacterFeatures with n=3\n",
      "Epoch 1: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 2: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 3: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 4: train error = 0.0000, validation error = 0.5000\n",
      "Epoch 5: train error = 0.0000, validation error = 0.5000\n",
      "Testing completed.\n",
      "\n",
      "\n",
      "--- Demo 6: K-Means Clustering ---\n",
      "Running K-Means with K=3 on 30 data points...\n",
      "Converged after 3 epochs.\n",
      "Final reconstruction loss: 180.1030\n",
      "\n",
      "Final cluster centers:\n",
      "  Cluster 1: {'x': 0.9852816271946256, 'y': 5.037113790791116}\n",
      "  Cluster 2: {'x': 3.0673241768076522, 'y': 2.9113985911744753}\n",
      "  Cluster 3: {'x': 0.6884550285727429, 'y': 4.777822985120473}\n",
      "\n",
      "Cluster assignments (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "--- Demo 7: Vector Utility Functions ---\n",
      "v1 = {'a': 1.0, 'b': 2.0}\n",
      "v2 = {'b': 3.0, 'c': 4.0}\n",
      "dotProduct(v1, v2) = 6.0\n",
      "\n",
      "Before increment: v1 = {'a': 1.0, 'b': 2.0}\n",
      "After increment(v1, 2.0, v2): v1 = {'a': 1.0, 'b': 8.0, 'c': 8.0}\n",
      "\n",
      "======================================================================\n",
      "ASSIGNMENT 2: ALL DEMOS COMPLETED SUCCESSFULLY\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# IMPORTING REQUIRED LIBRARIES\n",
    "# ------------------------------------------------------------\n",
    "import random\n",
    "from typing import Callable, Dict, List, Tuple, TypeVar\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TYPE DEFINITIONS\n",
    "# ------------------------------------------------------------\n",
    "FeatureVector = Dict[str, int]\n",
    "WeightVector = Dict[str, float]\n",
    "Example = Tuple[FeatureVector, int]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "# These helper functions are fundamental to the operations in this assignment.\n",
    "# They handle mathematical operations on sparse feature vectors and\n",
    "# support the core algorithms used in learning and clustering.\n",
    "# ============================================================\n",
    "\n",
    "def dotProduct(d1: Dict[str, float], d2: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    In this function, we compute the dot product between two sparse feature vectors.\n",
    "\n",
    "    Args:\n",
    "        d1 (dict): The first feature vector (e.g., {'good': 2, 'bad': 1}).\n",
    "        d2 (dict): The second feature vector (e.g., {'good': 1, 'bad': -1}).\n",
    "\n",
    "    Returns:\n",
    "        float: The computed dot product between the two vectors.\n",
    "    \"\"\"\n",
    "    return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "\n",
    "def increment(d1: Dict[str, float], scale: float, d2: Dict[str, float]) -> None:\n",
    "    \"\"\"\n",
    "    In this function, we update a sparse feature vector (d1) by adding a scaled version\n",
    "    of another vector (d2). This operation supports weight updates during learning.\n",
    "\n",
    "    Args:\n",
    "        d1 (dict): The dictionary to be updated.\n",
    "        scale (float): The scalar multiplier applied to d2.\n",
    "        d2 (dict): The dictionary providing update values.\n",
    "    \"\"\"\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + scale * v\n",
    "\n",
    "\n",
    "def evaluatePredictor(examples: List[Tuple[FeatureVector, int]],\n",
    "                      predictor: Callable[[FeatureVector], int]) -> float:\n",
    "    \"\"\"\n",
    "    In this function, we evaluate the accuracy of a trained predictor function\n",
    "    on a given dataset by calculating the fraction of misclassified examples.\n",
    "\n",
    "    Args:\n",
    "        examples (list): A list of (x, y) pairs where y ∈ {-1, +1}.\n",
    "        predictor (function): A function returning a predicted label for each x.\n",
    "\n",
    "    Returns:\n",
    "        float: The misclassification rate over all examples.\n",
    "    \"\"\"\n",
    "    error = sum(1 for x, y in examples if predictor(x) != y)\n",
    "    return error / len(examples)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3a: FEATURE EXTRACTION\n",
    "# ------------------------------------------------------------\n",
    "def extractWordFeatures(x: str) -> FeatureVector:\n",
    "    \"\"\"\n",
    "    In this function, we convert a text string into a sparse feature vector\n",
    "    where each unique word becomes a key and its count the corresponding value.\n",
    "\n",
    "    Args:\n",
    "        x (str): The input sentence (e.g., \"I am what I am\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A mapping from words to their counts.\n",
    "\n",
    "    Example:\n",
    "        Input:  \"I am what I am\"\n",
    "        Output: {'I': 2, 'am': 2, 'what': 1}\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    for word in x.split():  # Split text into individual words\n",
    "        features[word] = features.get(word, 0) + 1\n",
    "    return features\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3b: STOCHASTIC GRADIENT DESCENT (SGD)\n",
    "# ------------------------------------------------------------\n",
    "T = TypeVar('T')\n",
    "\n",
    "def learnPredictor(trainExamples: List[Tuple[T, int]],\n",
    "                   validationExamples: List[Tuple[T, int]],\n",
    "                   featureExtractor: Callable[[T], FeatureVector],\n",
    "                   numEpochs: int,\n",
    "                   eta: float) -> WeightVector:\n",
    "    \"\"\"\n",
    "    In this function, we implement stochastic gradient descent (SGD)\n",
    "    to train a linear classifier under the hinge loss criterion.\n",
    "\n",
    "    Args:\n",
    "        trainExamples: List of (x, y) pairs used for training.\n",
    "        validationExamples: List of (x, y) pairs used for validation.\n",
    "        featureExtractor: A function mapping x to its feature vector φ(x).\n",
    "        numEpochs: The number of training epochs to perform.\n",
    "        eta: The learning rate controlling update step size.\n",
    "\n",
    "    Returns:\n",
    "        WeightVector: The learned weights representing the classifier.\n",
    "    \"\"\"\n",
    "    weights: WeightVector = {}\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        for x, y in trainExamples:\n",
    "            phi = featureExtractor(x)\n",
    "            margin = dotProduct(weights, phi) * y\n",
    "            if margin < 1:\n",
    "                # Update weights to minimize hinge loss\n",
    "                increment(weights, eta * y, phi)\n",
    "\n",
    "        # Evaluate model performance after each epoch\n",
    "        trainError = evaluatePredictor(\n",
    "            trainExamples, lambda x: 1 if dotProduct(featureExtractor(x), weights) >= 0 else -1)\n",
    "        validationError = evaluatePredictor(\n",
    "            validationExamples, lambda x: 1 if dotProduct(featureExtractor(x), weights) >= 0 else -1)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: train error = {trainError:.4f}, validation error = {validationError:.4f}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3c: SYNTHETIC DATASET GENERATION\n",
    "# ------------------------------------------------------------\n",
    "def generateDataset(numExamples: int, weights: WeightVector) -> List[Example]:\n",
    "    \"\"\"\n",
    "    In this function, we generate synthetic examples consistent with\n",
    "    a given weight vector. Each example is labeled according to\n",
    "    the sign of its weighted sum.\n",
    "\n",
    "    Args:\n",
    "        numExamples (int): Number of examples to generate.\n",
    "        weights (dict): The true weight vector used for labeling.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (feature_vector, label).\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "\n",
    "    def generateExample() -> Example:\n",
    "        phi = {}\n",
    "        # Randomly assign integer features between 0 and 3\n",
    "        for f in weights:\n",
    "            phi[f] = random.randint(0, 3)\n",
    "        score = dotProduct(weights, phi)\n",
    "        y = 1 if score >= 0 else -1\n",
    "        return phi, y\n",
    "\n",
    "    return [generateExample() for _ in range(numExamples)]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3d: CHARACTER N-GRAM FEATURES\n",
    "# ------------------------------------------------------------\n",
    "def extractCharacterFeatures(n: int) -> Callable[[str], FeatureVector]:\n",
    "    \"\"\"\n",
    "    In this function, we define a feature extractor that produces\n",
    "    character-level n-gram features from input strings. Spaces are\n",
    "    removed to ensure contiguous n-gram generation.\n",
    "\n",
    "    Args:\n",
    "        n (int): The n-gram size (e.g., n=3 for trigrams).\n",
    "\n",
    "    Returns:\n",
    "        function: A function mapping strings to n-gram count dictionaries.\n",
    "    \"\"\"\n",
    "    def extract(x: str) -> FeatureVector:\n",
    "        x = x.replace(\" \", \"\").replace(\"\\t\", \"\")  # Remove whitespace\n",
    "        features = {}\n",
    "        for i in range(len(x) - n + 1):\n",
    "            gram = x[i:i + n]\n",
    "            features[gram] = features.get(gram, 0) + 1\n",
    "        return features\n",
    "\n",
    "    return extract\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3e: TESTING CHARACTER FEATURE VALUES\n",
    "# ------------------------------------------------------------\n",
    "def testValuesOfN(n: int):\n",
    "    \"\"\"\n",
    "    In this function, we test our character n-gram feature extractor\n",
    "    and learning algorithm using simple illustrative datasets.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the n-gram features to test.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTesting extractCharacterFeatures with n={n}\")\n",
    "\n",
    "    # Small illustrative datasets\n",
    "    trainExamples = [(\"good movie\", 1), (\"bad film\", -1)]\n",
    "    validationExamples = [(\"excellent show\", 1), (\"terrible play\", -1)]\n",
    "\n",
    "    featureExtractor = extractCharacterFeatures(n)\n",
    "    weights = learnPredictor(trainExamples, validationExamples, featureExtractor,\n",
    "                             numEpochs=5, eta=0.01)\n",
    "    print(\"Testing completed.\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 5: K-MEANS CLUSTERING\n",
    "# ------------------------------------------------------------\n",
    "def kmeans(examples: List[Dict[str, float]], K: int, maxEpochs: int):\n",
    "    \"\"\"\n",
    "    In this function, we implement the K-Means clustering algorithm\n",
    "    for sparse feature vectors. The algorithm alternates between\n",
    "    assigning examples to the nearest cluster and updating centroids.\n",
    "\n",
    "    Args:\n",
    "        examples (list): List of feature vectors (dicts).\n",
    "        K (int): Number of clusters.\n",
    "        maxEpochs (int): Maximum number of iterations before termination.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (centers, assignments, final_loss)\n",
    "    \"\"\"\n",
    "    # Randomly select K initial cluster centers\n",
    "    centers = random.sample(examples, K)\n",
    "    assignments = [0] * len(examples)\n",
    "\n",
    "    def squaredDistance(a, b):\n",
    "        \"\"\"Helper function to compute squared Euclidean distance.\"\"\"\n",
    "        diff = {}\n",
    "        increment(diff, 1, a)\n",
    "        increment(diff, -1, b)\n",
    "        return dotProduct(diff, diff)\n",
    "\n",
    "    for epoch in range(maxEpochs):\n",
    "        # Step 1: Assign examples to the nearest cluster\n",
    "        newAssignments = []\n",
    "        for ex in examples:\n",
    "            distances = [squaredDistance(ex, c) for c in centers]\n",
    "            newAssignments.append(distances.index(min(distances)))\n",
    "\n",
    "        # Step 2: Check for convergence\n",
    "        if newAssignments == assignments:\n",
    "            print(f\"Converged after {epoch + 1} epochs.\")\n",
    "            break\n",
    "        assignments = newAssignments\n",
    "\n",
    "        # Step 3: Update cluster centroids\n",
    "        newCenters = [{} for _ in range(K)]\n",
    "        counts = [0] * K\n",
    "        for i, ex in enumerate(examples):\n",
    "            j = assignments[i]\n",
    "            increment(newCenters[j], 1, ex)\n",
    "            counts[j] += 1\n",
    "        for j in range(K):\n",
    "            if counts[j] > 0:\n",
    "                for f in newCenters[j]:\n",
    "                    newCenters[j][f] /= counts[j]\n",
    "        centers = newCenters\n",
    "\n",
    "    # Step 4: Compute reconstruction loss\n",
    "    loss = 0.0\n",
    "    for i, ex in enumerate(examples):\n",
    "        loss += squaredDistance(ex, centers[assignments[i]])\n",
    "\n",
    "    print(f\"Final reconstruction loss: {loss:.4f}\")\n",
    "    return centers, assignments, loss\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN BLOCK - Comprehensive Testing and Demonstrations\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 2: SENTIMENT CLASSIFICATION - COMPREHENSIVE DEMOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Demo 1: Feature Extraction\n",
    "    print(\"\\n--- Demo 1: Word Feature Extraction ---\")\n",
    "    test_sentences = [\n",
    "        \"I am what I am\",\n",
    "        \"good movie great film\",\n",
    "        \"Python programming is awesome\"\n",
    "    ]\n",
    "    for sent in test_sentences:\n",
    "        features = extractWordFeatures(sent)\n",
    "        print(f\"Text: '{sent}'\")\n",
    "        print(f\"Features: {features}\\n\")\n",
    "    \n",
    "    # Demo 2: Stochastic Gradient Descent Learning\n",
    "    print(\"\\n--- Demo 2: Sentiment Classification with SGD ---\")\n",
    "    print(\"Training simple sentiment classifier...\")\n",
    "    trainExamples = [\n",
    "        (\"excellent movie\", 1),\n",
    "        (\"great film\", 1),\n",
    "        (\"wonderful show\", 1),\n",
    "        (\"bad movie\", -1),\n",
    "        (\"terrible film\", -1),\n",
    "        (\"awful show\", -1)\n",
    "    ]\n",
    "    validationExamples = [\n",
    "        (\"amazing performance\", 1),\n",
    "        (\"horrible acting\", -1)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Training on {len(trainExamples)} examples...\")\n",
    "    weights = learnPredictor(trainExamples, validationExamples, \n",
    "                            extractWordFeatures, numEpochs=10, eta=0.1)\n",
    "    print(f\"\\nLearned weights (top 10): {dict(sorted(weights.items(), key=lambda x: abs(x[1]), reverse=True)[:10])}\")\n",
    "    \n",
    "    # Test predictions\n",
    "    print(\"\\n--- Testing Predictions ---\")\n",
    "    test_texts = [\"great performance\", \"bad acting\", \"excellent show\"]\n",
    "    for text in test_texts:\n",
    "        features = extractWordFeatures(text)\n",
    "        score = dotProduct(weights, features)\n",
    "        prediction = \"positive\" if score >= 0 else \"negative\"\n",
    "        print(f\"'{text}' -> score: {score:.4f}, prediction: {prediction}\")\n",
    "    \n",
    "    # Demo 3: Synthetic Dataset Generation\n",
    "    print(\"\\n--- Demo 3: Synthetic Dataset Generation ---\")\n",
    "    true_weights = {\"feature1\": 2.0, \"feature2\": -1.5, \"feature3\": 0.5}\n",
    "    print(f\"True weights: {true_weights}\")\n",
    "    dataset = generateDataset(20, true_weights)\n",
    "    print(f\"Generated {len(dataset)} examples:\")\n",
    "    for i, (phi, y) in enumerate(dataset[:5]):\n",
    "        print(f\"  Example {i+1}: features={phi}, label={y}\")\n",
    "    print(f\"  ... (showing first 5 of {len(dataset)})\")\n",
    "    \n",
    "    # Demo 4: Character N-Gram Features\n",
    "    print(\"\\n--- Demo 4: Character N-Gram Features ---\")\n",
    "    for n in [2, 3, 4]:\n",
    "        extractor = extractCharacterFeatures(n)\n",
    "        test_text = \"hello world\"\n",
    "        features = extractor(test_text)\n",
    "        print(f\"n={n}, text='{test_text}', {n}-grams: {features}\")\n",
    "    \n",
    "    # Demo 5: Testing Different N Values for Learning\n",
    "    print(\"\\n--- Demo 5: Learning with Character N-Grams ---\")\n",
    "    for n in [2, 3]:\n",
    "        testValuesOfN(n)\n",
    "    \n",
    "    # Demo 6: K-Means Clustering\n",
    "    print(\"\\n--- Demo 6: K-Means Clustering ---\")\n",
    "    # Generate synthetic data for clustering\n",
    "    random.seed(42)\n",
    "    cluster_data = []\n",
    "    # Cluster 1: centered around {\"x\": 1, \"y\": 1}\n",
    "    for _ in range(10):\n",
    "        cluster_data.append({\"x\": random.uniform(0.5, 1.5), \"y\": random.uniform(0.5, 1.5)})\n",
    "    # Cluster 2: centered around {\"x\": 5, \"y\": 5}\n",
    "    for _ in range(10):\n",
    "        cluster_data.append({\"x\": random.uniform(4.5, 5.5), \"y\": random.uniform(4.5, 5.5)})\n",
    "    # Cluster 3: centered around {\"x\": 1, \"y\": 5}\n",
    "    for _ in range(10):\n",
    "        cluster_data.append({\"x\": random.uniform(0.5, 1.5), \"y\": random.uniform(4.5, 5.5)})\n",
    "    \n",
    "    print(f\"Running K-Means with K=3 on {len(cluster_data)} data points...\")\n",
    "    centers, assignments, loss = kmeans(cluster_data, K=3, maxEpochs=20)\n",
    "    \n",
    "    print(f\"\\nFinal cluster centers:\")\n",
    "    for i, center in enumerate(centers):\n",
    "        print(f\"  Cluster {i+1}: {center}\")\n",
    "    \n",
    "    print(f\"\\nCluster assignments (first 10): {assignments[:10]}\")\n",
    "    \n",
    "    # Demo 7: Utility Functions\n",
    "    print(\"\\n--- Demo 7: Vector Utility Functions ---\")\n",
    "    v1 = {\"a\": 1.0, \"b\": 2.0}\n",
    "    v2 = {\"b\": 3.0, \"c\": 4.0}\n",
    "    print(f\"v1 = {v1}\")\n",
    "    print(f\"v2 = {v2}\")\n",
    "    print(f\"dotProduct(v1, v2) = {dotProduct(v1, v2)}\")\n",
    "    \n",
    "    v1_copy = dict(v1)\n",
    "    print(f\"\\nBefore increment: v1 = {v1_copy}\")\n",
    "    increment(v1_copy, 2.0, v2)\n",
    "    print(f\"After increment(v1, 2.0, v2): v1 = {v1_copy}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ASSIGNMENT 2: ALL DEMOS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a8ea6-5f4d-4268-8ca6-9126f4995369",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 3\n",
    "# Title: Route Planning\n",
    "## File Purpose\n",
    "This notebook implements various search-based problems for **Route Planning**.  \n",
    "Students are required to model states and transitions effectively to solve:  \n",
    "- Shortest path problems  \n",
    "- Waypoints shortest path problems  \n",
    "- A* search reductions and heuristics  \n",
    "\n",
    "The implementations use the provided utility classes:  \n",
    "- `CityMap`  \n",
    "- `State`  \n",
    "- `SearchProblem`  \n",
    "- `UniformCostSearch`  \n",
    "- `Heuristic`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518a0fa-f582-40f6-af73-6f12232796df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import sys\n",
    "sys.path.insert(0, 'route')  # Add route folder to Python path\n",
    "\n",
    "from mapUtil import (\n",
    "    CityMap,\n",
    "    computeDistance,\n",
    "    createStanfordMap,\n",
    "    locationFromTag,\n",
    "    makeTag,\n",
    "    getTotalCost,\n",
    ")\n",
    "from util import Heuristic, SearchProblem, State, UniformCostSearch\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# IMPORTANT NOTE ON STATE REPRESENTATION\n",
    "# ============================================================\n",
    "# Each search problem must define what information is stored in `State.memory`.\n",
    "# This determines how efficiently the algorithm searches through routes.\n",
    "# For example:\n",
    "#   - For simple shortest path: only `location` is needed.\n",
    "#   - For waypoint problems: `memory` should track covered tags, not full paths.\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 2a: MODELING THE SHORTEST PATH PROBLEM\n",
    "# ============================================================\n",
    "\n",
    "class ShortestPathProblem(SearchProblem):\n",
    "    \"\"\"\n",
    "    A search problem for finding the shortest path from a start location to\n",
    "    any location with the specified end tag (e.g., 'amenity=food').\n",
    "\n",
    "    Attributes:\n",
    "        startLocation (str): Starting node on the map.\n",
    "        endTag (str): Destination tag to reach (e.g., 'landmark=gates').\n",
    "        cityMap (CityMap): The map data structure representing all locations,\n",
    "                           distances, and tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, startLocation: str, endTag: str, cityMap: CityMap):\n",
    "        self.startLocation = startLocation\n",
    "        self.endTag = endTag\n",
    "        self.cityMap = cityMap\n",
    "\n",
    "    def startState(self) -> State:\n",
    "        \"\"\"\n",
    "        Defines the initial state of the search.\n",
    "\n",
    "        Returns:\n",
    "            State: A new state with only the start location.\n",
    "        \"\"\"\n",
    "        return State(location=self.startLocation)\n",
    "\n",
    "    def isEnd(self, state: State) -> bool:\n",
    "        \"\"\"\n",
    "        Checks whether the current state corresponds to a goal location.\n",
    "\n",
    "        Args:\n",
    "            state (State): The current state in the search.\n",
    "        Returns:\n",
    "            bool: True if the location has the target endTag.\n",
    "        \"\"\"\n",
    "        return self.endTag in self.cityMap.tags[state.location]\n",
    "\n",
    "    def actionSuccessorsAndCosts(self, state: State) -> List[Tuple[str, State, float]]:\n",
    "        \"\"\"\n",
    "        Returns possible actions and resulting states from the current location.\n",
    "\n",
    "        Each action is a transition to a neighboring location, with a cost equal\n",
    "        to the distance between them.\n",
    "\n",
    "        Args:\n",
    "            state (State): The current location/state.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, State, float]]:\n",
    "                A list of (action, nextState, cost) tuples.\n",
    "        \"\"\"\n",
    "        successors = []\n",
    "        for nextLocation, cost in self.cityMap.distances[state.location].items():\n",
    "            successorState = State(location=nextLocation)\n",
    "            successors.append((nextLocation, successorState, cost))\n",
    "        return successors\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 2b: CUSTOM — PLAN A ROUTE THROUGH STANFORD\n",
    "# ============================================================\n",
    "\n",
    "def getStanfordShortestPathProblem() -> ShortestPathProblem:\n",
    "    \"\"\"\n",
    "    Creates a sample route planning problem using the Stanford map.\n",
    "\n",
    "    This example finds the shortest path from any starting location to\n",
    "    any location tagged with `amenity=food`.\n",
    "\n",
    "    Returns:\n",
    "        ShortestPathProblem: A configured instance for Stanford map route planning.\n",
    "    \"\"\"\n",
    "    cityMap = createStanfordMap()\n",
    "    startLocation = next(iter(cityMap.tags.keys()))  # Choose the first known location\n",
    "    endTag = makeTag(\"amenity\", \"food\")\n",
    "    return ShortestPathProblem(startLocation, endTag, cityMap)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3a: MODELING THE WAYPOINTS SHORTEST PATH PROBLEM\n",
    "# ============================================================\n",
    "\n",
    "class WaypointsShortestPathProblem(SearchProblem):\n",
    "    \"\"\"\n",
    "    A route planning problem requiring traversal through multiple waypoints\n",
    "    before reaching a destination with a given endTag.\n",
    "\n",
    "    Attributes:\n",
    "        startLocation (str): The initial position.\n",
    "        waypointTags (tuple): Tags of required waypoints to visit.\n",
    "        endTag (str): Destination tag to reach.\n",
    "        cityMap (CityMap): The map structure containing distances and tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, startLocation: str, waypointTags: List[str],\n",
    "                 endTag: str, cityMap: CityMap):\n",
    "        self.startLocation = startLocation\n",
    "        self.endTag = endTag\n",
    "        self.cityMap = cityMap\n",
    "        self.waypointTags = tuple(sorted(waypointTags))\n",
    "\n",
    "        # Precompute relevant locations for efficiency\n",
    "        self.locations_with_tags = {\n",
    "            tag: frozenset(loc for loc, tags in cityMap.tags.items() if tag in tags)\n",
    "            for tag in self.waypointTags\n",
    "        }\n",
    "        self.end_locations = frozenset(\n",
    "            loc for loc, tags in cityMap.tags.items() if endTag in tags\n",
    "        )\n",
    "\n",
    "    def startState(self) -> State:\n",
    "        \"\"\"\n",
    "        Initializes the state with the starting location and any waypoint\n",
    "        tags already covered there.\n",
    "\n",
    "        Returns:\n",
    "            State: Initial state containing location and memory (covered tags).\n",
    "        \"\"\"\n",
    "        initial_covered = frozenset(\n",
    "            tag for tag in self.waypointTags\n",
    "            if tag in self.cityMap.tags[self.startLocation]\n",
    "        )\n",
    "        return State(location=self.startLocation, memory=initial_covered)\n",
    "\n",
    "    def isEnd(self, state: State) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if we are at a valid end location *and* have visited all waypoints.\n",
    "\n",
    "        Args:\n",
    "            state (State): Current state of the search.\n",
    "        Returns:\n",
    "            bool: True if end conditions are satisfied.\n",
    "        \"\"\"\n",
    "        return (state.location in self.end_locations and\n",
    "                len(state.memory) == len(self.waypointTags))\n",
    "\n",
    "    def actionSuccessorsAndCosts(self, state: State) -> List[Tuple[str, State, float]]:\n",
    "        \"\"\"\n",
    "        Generates successor states from the current state considering waypoint constraints.\n",
    "\n",
    "        - If all waypoints are collected, only allow moves to end locations.\n",
    "        - Otherwise, explore paths that may lead to new waypoints.\n",
    "\n",
    "        Args:\n",
    "            state (State): The current location and memory state.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, State, float]]: Possible (action, newState, cost) tuples.\n",
    "        \"\"\"\n",
    "        successors = []\n",
    "        have_all_waypoints = len(state.memory) == len(self.waypointTags)\n",
    "\n",
    "        for nextLocation, cost in self.cityMap.distances[state.location].items():\n",
    "            # If all waypoints covered, move only toward valid end locations\n",
    "            if have_all_waypoints and nextLocation not in self.end_locations:\n",
    "                continue\n",
    "\n",
    "            # If still missing waypoints, check if this next location adds new ones\n",
    "            if not have_all_waypoints:\n",
    "                new_tags = frozenset(\n",
    "                    tag for tag in self.waypointTags\n",
    "                    if tag not in state.memory and nextLocation in self.locations_with_tags[tag]\n",
    "                )\n",
    "                if not new_tags:\n",
    "                    continue  # Skip irrelevant locations\n",
    "                covered_tags = frozenset(state.memory | new_tags)\n",
    "            else:\n",
    "                covered_tags = state.memory\n",
    "\n",
    "            successorState = State(location=nextLocation, memory=covered_tags)\n",
    "            successors.append((nextLocation, successorState, cost))\n",
    "\n",
    "        return successors\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3c: CUSTOM — PLAN A ROUTE WITH WAYPOINTS\n",
    "# ============================================================\n",
    "\n",
    "def getStanfordWaypointsShortestPathProblem() -> WaypointsShortestPathProblem:\n",
    "    \"\"\"\n",
    "    Defines a route planning problem through Stanford requiring waypoints.\n",
    "\n",
    "    Example route:\n",
    "      - Start: Gates Building\n",
    "      - Must visit: Food and Library locations\n",
    "      - End: Parking\n",
    "\n",
    "    Returns:\n",
    "        WaypointsShortestPathProblem: A configured waypoint problem instance.\n",
    "    \"\"\"\n",
    "    cityMap = createStanfordMap()\n",
    "    gates_tag = makeTag(\"landmark\", \"gates\")\n",
    "    startLocation = locationFromTag(gates_tag, cityMap)\n",
    "    if startLocation is None:\n",
    "        raise ValueError(\"Could not find Gates building location\")\n",
    "\n",
    "    waypointTags = [makeTag(\"amenity\", \"food\"), makeTag(\"amenity\", \"library\")]\n",
    "    endTag = makeTag(\"amenity\", \"parking\")\n",
    "\n",
    "    return WaypointsShortestPathProblem(startLocation, waypointTags, endTag, cityMap)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 4a: A* → UCS REDUCTION\n",
    "# ============================================================\n",
    "\n",
    "def aStarReduction(problem: SearchProblem, heuristic: Heuristic) -> SearchProblem:\n",
    "    \"\"\"\n",
    "    Reduces an A* search problem into an equivalent Uniform Cost Search problem.\n",
    "\n",
    "    The idea:\n",
    "        - A* modifies transition costs to incorporate heuristic estimates.\n",
    "        - This function builds a new problem with these modified costs.\n",
    "\n",
    "    Args:\n",
    "        problem (SearchProblem): The original problem definition.\n",
    "        heuristic (Heuristic): The heuristic function h(s).\n",
    "\n",
    "    Returns:\n",
    "        SearchProblem: An equivalent UCS-style problem using adjusted costs.\n",
    "    \"\"\"\n",
    "    class NewSearchProblem(SearchProblem):\n",
    "        def startState(self) -> State:\n",
    "            return problem.startState()\n",
    "\n",
    "        def isEnd(self, state: State) -> bool:\n",
    "            return problem.isEnd(state)\n",
    "\n",
    "        def actionSuccessorsAndCosts(self, state: State) -> List[Tuple[str, State, float]]:\n",
    "            successors = []\n",
    "            for action, nextState, cost in problem.actionSuccessorsAndCosts(state):\n",
    "                # Modify cost according to heuristic difference\n",
    "                h_cost = heuristic.evaluate(nextState)\n",
    "                successors.append((action, nextState, cost + h_cost - heuristic.evaluate(state)))\n",
    "            return successors\n",
    "\n",
    "    return NewSearchProblem()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 4b: STRAIGHT-LINE HEURISTIC FOR A*\n",
    "# ============================================================\n",
    "\n",
    "class StraightLineHeuristic(Heuristic):\n",
    "    \"\"\"\n",
    "    Estimates the cost from the current state to any goal state using\n",
    "    the straight-line (Euclidean) distance between coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endTag: str, cityMap: CityMap):\n",
    "        self.endTag = endTag\n",
    "        self.cityMap = cityMap\n",
    "        self.endLocations = [\n",
    "            location for location in cityMap.tags if endTag in cityMap.tags[location]\n",
    "        ]\n",
    "\n",
    "    def evaluate(self, state: State) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the minimum straight-line distance between the current\n",
    "        location and all possible goal locations.\n",
    "\n",
    "        Returns:\n",
    "            float: Minimum straight-line distance estimate.\n",
    "        \"\"\"\n",
    "        if not self.endLocations:\n",
    "            return 0\n",
    "        minDistance = float(\"inf\")\n",
    "        for endLoc in self.endLocations:\n",
    "            dist = computeDistance(\n",
    "                self.cityMap.latLong[state.location],\n",
    "                self.cityMap.latLong[endLoc]\n",
    "            )\n",
    "            minDistance = min(minDistance, dist)\n",
    "        return minDistance\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 4c: NO-WAYPOINTS HEURISTIC FOR A*\n",
    "# ============================================================\n",
    "\n",
    "class NoWaypointsHeuristic(Heuristic):\n",
    "    \"\"\"\n",
    "    Heuristic that precomputes the minimal cost from each location\n",
    "    to any destination with the target `endTag`, ignoring waypoint constraints.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endTag: str, cityMap: CityMap):\n",
    "        \"\"\"\n",
    "        Uses Uniform Cost Search to compute exact costs from every location\n",
    "        to any location with the end tag, assuming symmetric distances.\n",
    "        \"\"\"\n",
    "        class ReverseShortestPathProblem(SearchProblem):\n",
    "            def startState(self) -> State:\n",
    "                return State(location=\"END\")\n",
    "\n",
    "            def isEnd(self, state: State) -> bool:\n",
    "                return False  # No single end state — explore all locations\n",
    "\n",
    "            def actionSuccessorsAndCosts(self, state: State) -> List[Tuple[str, State, float]]:\n",
    "                successors = []\n",
    "                if state.location == \"END\":\n",
    "                    # Connect END node to all goal locations at zero cost\n",
    "                    for location in cityMap.tags:\n",
    "                        if endTag in cityMap.tags[location]:\n",
    "                            successors.append((location, State(location=location), 0))\n",
    "                else:\n",
    "                    for nextLocation, cost in cityMap.distances[state.location].items():\n",
    "                        successors.append((nextLocation, State(location=nextLocation), cost))\n",
    "                return successors\n",
    "\n",
    "        reverseProblem = ReverseShortestPathProblem()\n",
    "        ucs = UniformCostSearch()\n",
    "        ucs.solve(reverseProblem)\n",
    "        self.pastCosts = ucs.pastCosts  # Store minimal cost per state\n",
    "\n",
    "    def evaluate(self, state: State) -> float:\n",
    "        \"\"\"\n",
    "        Retrieves the precomputed cost-to-go from this state to a goal.\n",
    "\n",
    "        Args:\n",
    "            state (State): Current state to evaluate.\n",
    "        Returns:\n",
    "            float: Precomputed shortest path cost (∞ if unknown).\n",
    "        \"\"\"\n",
    "        return self.pastCosts.get(state, float(\"inf\"))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN: RUN ROUTE PLANNING DEMOS\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Always run in notebook\n",
    "    print(\"=\"*70)\n",
    "    print(\"ROUTE PLANNING DEMOS - AI ASSIGNMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Demo 1: Shortest Path with UCS\n",
    "        print(\"\\n[Demo 1] Shortest Path Problem (UCS)\")\n",
    "        print(\"-\" * 70)\n",
    "        problem1 = getStanfordShortestPathProblem()\n",
    "        ucs1 = UniformCostSearch()\n",
    "        ucs1.solve(problem1)\n",
    "        \n",
    "        if ucs1.pathCost is not None:\n",
    "            path1 = [problem1.startLocation] + ucs1.actions\n",
    "            print(f\"✓ Found path from {problem1.startLocation[:30]}...\")\n",
    "            print(f\"  to a location with tag: {problem1.endTag}\")\n",
    "            print(f\"  Path length: {len(path1)} locations\")\n",
    "            print(f\"  Total cost: {getTotalCost(path1, problem1.cityMap):.2f}\")\n",
    "            print(f\"  States explored: {ucs1.numStatesExplored}\")\n",
    "        else:\n",
    "            print(\"✗ No path found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    try:\n",
    "        # Demo 2: Shortest Path with A* (using StraightLineHeuristic)\n",
    "        print(\"\\n[Demo 2] Shortest Path Problem (A* with Straight-Line Heuristic)\")\n",
    "        print(\"-\" * 70)\n",
    "        problem2 = getStanfordShortestPathProblem()\n",
    "        heuristic2 = StraightLineHeuristic(problem2.endTag, problem2.cityMap)\n",
    "        astar_problem2 = aStarReduction(problem2, heuristic2)\n",
    "        ucs2 = UniformCostSearch()\n",
    "        ucs2.solve(astar_problem2)\n",
    "        \n",
    "        if ucs2.pathCost is not None:\n",
    "            path2 = [problem2.startLocation] + ucs2.actions\n",
    "            print(f\"✓ Found path from {problem2.startLocation[:30]}...\")\n",
    "            print(f\"  to a location with tag: {problem2.endTag}\")\n",
    "            print(f\"  Path length: {len(path2)} locations\")\n",
    "            print(f\"  Total cost: {getTotalCost(path2, problem2.cityMap):.2f}\")\n",
    "            print(f\"  States explored: {ucs2.numStatesExplored}\")\n",
    "            print(f\"  Improvement over UCS: {ucs1.numStatesExplored - ucs2.numStatesExplored} fewer states\")\n",
    "        else:\n",
    "            print(\"✗ No path found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    try:\n",
    "        # Demo 3: Waypoints Problem with UCS\n",
    "        print(\"\\n[Demo 3] Waypoints Shortest Path Problem (UCS)\")\n",
    "        print(\"-\" * 70)\n",
    "        problem3 = getStanfordWaypointsShortestPathProblem()\n",
    "        ucs3 = UniformCostSearch()\n",
    "        ucs3.solve(problem3)\n",
    "        \n",
    "        if ucs3.pathCost is not None:\n",
    "            path3 = [problem3.startLocation] + ucs3.actions\n",
    "            print(f\"✓ Found path from {problem3.startLocation[:30]}...\")\n",
    "            print(f\"  through waypoints: {problem3.waypointTags}\")\n",
    "            print(f\"  to a location with tag: {problem3.endTag}\")\n",
    "            print(f\"  Path length: {len(path3)} locations\")\n",
    "            print(f\"  Total cost: {getTotalCost(path3, problem3.cityMap):.2f}\")\n",
    "            print(f\"  States explored: {ucs3.numStatesExplored}\")\n",
    "            \n",
    "            # Show waypoints covered\n",
    "            print(\"\\n  Waypoints covered along path:\")\n",
    "            covered = set()\n",
    "            for loc in path3:\n",
    "                for tag in problem3.cityMap.tags.get(loc, []):\n",
    "                    if tag in problem3.waypointTags and tag not in covered:\n",
    "                        print(f\"    - {tag} at {loc[:40]}...\")\n",
    "                        covered.add(tag)\n",
    "        else:\n",
    "            print(\"✗ No path found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    try:\n",
    "        # Demo 4: Waypoints Problem with A* (using NoWaypointsHeuristic)\n",
    "        print(\"\\n[Demo 4] Waypoints Problem (A* with No-Waypoints Heuristic)\")\n",
    "        print(\"-\" * 70)\n",
    "        problem4 = getStanfordWaypointsShortestPathProblem()\n",
    "        print(\"  Precomputing heuristic (this may take a moment)...\")\n",
    "        heuristic4 = NoWaypointsHeuristic(problem4.endTag, problem4.cityMap)\n",
    "        astar_problem4 = aStarReduction(problem4, heuristic4)\n",
    "        ucs4 = UniformCostSearch()\n",
    "        ucs4.solve(astar_problem4)\n",
    "        \n",
    "        if ucs4.pathCost is not None:\n",
    "            path4 = [problem4.startLocation] + ucs4.actions\n",
    "            print(f\"✓ Found path from {problem4.startLocation[:30]}...\")\n",
    "            print(f\"  through waypoints: {problem4.waypointTags}\")\n",
    "            print(f\"  to a location with tag: {problem4.endTag}\")\n",
    "            print(f\"  Path length: {len(path4)} locations\")\n",
    "            print(f\"  Total cost: {getTotalCost(path4, problem4.cityMap):.2f}\")\n",
    "            print(f\"  States explored: {ucs4.numStatesExplored}\")\n",
    "            print(f\"  Improvement over UCS: {ucs3.numStatesExplored - ucs4.numStatesExplored} fewer states\")\n",
    "        else:\n",
    "            print(\"✗ No path found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL ROUTE PLANNING DEMOS COMPLETED\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd37d8-f36c-4a7b-abc0-a16cb6e1ec65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 4\n",
    "## Title: Controlling Mountain Car\n",
    "**Description:**  \n",
    "This assignment implements reinforcement learning algorithms for controlling the Mountain Car environment, including:  \n",
    "- Value Iteration on a tabular MDP  \n",
    "- Model-Based Monte Carlo  \n",
    "- Tabular Q-Learning  \n",
    "- Function Approximation Q-Learning (Fourier Features)  \n",
    "- Constrained Q-Learning enforcing velocity constraints  \n",
    "\n",
    "Each algorithm is implemented with clarity and proper documentation to ensure reproducibility and understanding of reinforcement learning principles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1a759-f9c9-4cf5-aa32-b31c18dcc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard libraries\n",
    "import math, random\n",
    "from collections import defaultdict\n",
    "from typing import List, Callable, Tuple, Dict, Any, Optional, Iterable, Set\n",
    "\n",
    "# Third-party libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Local utilities provided as part of the assignment\n",
    "import util\n",
    "from util import ContinuousGymMDP, StateT, ActionT\n",
    "from custom_mountain_car import CustomMountainCarEnv\n",
    "\n",
    "############################################################\n",
    "# ### Problem 3a: Value Iteration\n",
    "# Implementing Value Iteration on Number Line (from Problem 1)\n",
    "############################################################\n",
    "\n",
    "def valueIteration(succAndRewardProb: Dict[Tuple[StateT, ActionT], List[Tuple[StateT, float, float]]],\n",
    "                   discount: float,\n",
    "                   epsilon: float = 0.001):\n",
    "    \"\"\"\n",
    "    Perform Value Iteration on a tabular MDP given an explicit model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    succAndRewardProb : dict\n",
    "        Mapping (state, action) -> list of (nextState, prob, reward) triples.\n",
    "        This encodes the transition probability and reward model for the MDP.\n",
    "    discount : float\n",
    "        Discount factor gamma in [0, 1).\n",
    "    epsilon : float, optional\n",
    "        Convergence tolerance for value iteration. Iteration stops when the\n",
    "        maximum change in the value function across states is < epsilon.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    policy : dict\n",
    "        A mapping from state -> optimal action (greedy w.r.t. computed V).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function computes the optimal value function via synchronous\n",
    "      updates (standard Bellman optimality backup), and then returns the\n",
    "      greedy policy. Terminal states should not appear with actions in the\n",
    "      `succAndRewardProb` dictionary; states not appearing default to value 0.\n",
    "    - We preserve tie-breaking behavior: when multiple actions have equal Q,\n",
    "      the action with larger action id is chosen (consistent deterministic tie\n",
    "      breaking helps reproducibility of results).\n",
    "    \"\"\"\n",
    "\n",
    "    # Build a mapping from each state to the set of actions seen in the model.\n",
    "    # This lets us iterate states (and handle states with no outgoing actions).\n",
    "    stateActions = defaultdict(set)\n",
    "    for state, action in succAndRewardProb.keys():\n",
    "        stateActions[state].add(action)\n",
    "\n",
    "    def computeQ(V: Dict[StateT, float], state: StateT, action: ActionT) -> float:\n",
    "        \"\"\"Return Q(s,a) under current value estimate V using model-based backup.\n",
    "\n",
    "        Q(s,a) = sum_{s'} P(s'|s,a) * (R(s,a,s') + discount * V(s'))\n",
    "        \"\"\"\n",
    "        return sum(prob * (reward + discount * V[nextState]) for nextState, prob, reward in succAndRewardProb[(state, action)])\n",
    "\n",
    "    def computePolicy(V: Dict[StateT, float]) -> Dict[StateT, ActionT]:\n",
    "        \"\"\"Return greedy policy mapping state -> argmax_a Q(s,a).\n",
    "\n",
    "        Tie-breaking: when Q-values are equal, choose action with larger value.\n",
    "        \"\"\"\n",
    "        policy = {}\n",
    "        for state in stateActions:\n",
    "            # Choose action with maximal Q (tie-break on action id)\n",
    "            policy[state] = max(stateActions[state], key=lambda a: (computeQ(V, state, a), a))\n",
    "        return policy\n",
    "\n",
    "    print('Running valueIteration...')\n",
    "\n",
    "    # Use defaultdict(float) so missing states default to 0 (useful for terminal)\n",
    "    V = defaultdict(float)\n",
    "    numIters = 0\n",
    "\n",
    "    # Synchronous value iteration loop\n",
    "    while True:\n",
    "        newV = defaultdict(float)\n",
    "        maxDiff = 0\n",
    "        for state in stateActions:\n",
    "            if stateActions[state]:  # Only perform update for states with actions\n",
    "                # Bellman optimality: V(s) = max_a Q(s,a) using current V\n",
    "                newV[state] = max(computeQ(V, state, action) for action in stateActions[state])\n",
    "                # Track max update magnitude for convergence check\n",
    "                maxDiff = max(maxDiff, abs(newV[state] - V[state]))\n",
    "        # Check for termination\n",
    "        if maxDiff < epsilon:\n",
    "            break\n",
    "        # Otherwise continue iterating with the updated values\n",
    "        V = newV\n",
    "        numIters += 1\n",
    "\n",
    "    V_opt = V\n",
    "    print((\"valueIteration: %d iterations\" % numIters))\n",
    "\n",
    "    # Return greedy policy computed from final V\n",
    "    return computePolicy(V_opt)\n",
    "\n",
    "############################################################\n",
    "# ### Problem 3b: Model-Based Monte Carlo\n",
    "# Build a model from samples and run Value Iteration periodically.\n",
    "############################################################\n",
    "\n",
    "\n",
    "def run_VI_over_numberLine(mdp: util.NumberLineMDP):\n",
    "    \"\"\"\n",
    "    Construct the explicit transition/reward model for the NumberLine MDP\n",
    "    used in Problem 1 and run value iteration to obtain the optimal policy.\n",
    "\n",
    "    This helper demonstrates how to construct succAndRewardProb for a\n",
    "    small tabular MDP (the number line) and then run the valueIteration\n",
    "    function above to compute the optimal policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mdp : util.NumberLineMDP\n",
    "        The number-line MDP object provided by the assignment utilities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : dict\n",
    "        Optimal policy mapping from state -> action for the modeled MDP.\n",
    "    \"\"\"\n",
    "\n",
    "    # Edge transitions (near terminal states) have different probabilities\n",
    "    succAndRewardProb = {\n",
    "        (-mdp.n + 1, 1): [(-mdp.n + 2, 0.2, mdp.penalty), (-mdp.n, 0.8, mdp.leftReward)],\n",
    "        (-mdp.n + 1, 2): [(-mdp.n + 2, 0.3, mdp.penalty), (-mdp.n, 0.7, mdp.leftReward)],\n",
    "        (mdp.n - 1, 1): [(mdp.n - 2, 0.8, mdp.penalty), (mdp.n, 0.2, mdp.rightReward)],\n",
    "        (mdp.n - 1, 2): [(mdp.n - 2, 0.7, mdp.penalty), (mdp.n, 0.3, mdp.rightReward)]\n",
    "    }\n",
    "\n",
    "    # Interior states have identical structure across the range; fill them in\n",
    "    for s in range(-mdp.n + 2, mdp.n - 1):\n",
    "        succAndRewardProb[(s, 1)] = [(s+1, 0.2, mdp.penalty), (s - 1, 0.8, mdp.penalty)]\n",
    "        succAndRewardProb[(s, 2)] = [(s+1, 0.3, mdp.penalty), (s - 1, 0.7, mdp.penalty)]\n",
    "\n",
    "    # Run value iteration on the constructed model\n",
    "    pi = valueIteration(succAndRewardProb, mdp.discount)\n",
    "    return pi\n",
    "\n",
    "\n",
    "class ModelBasedMonteCarlo(util.RLAlgorithm):\n",
    "    \"\"\"\n",
    "    A simple model-based Monte Carlo algorithm that estimates the transition\n",
    "    probabilities and rewards from observed data and periodically runs value\n",
    "    iteration on the estimated model to compute a greedy policy.\n",
    "\n",
    "    Data structures\n",
    "    ---------------\n",
    "    - tCounts[(s,a)][s'] : int\n",
    "        Count of observed transitions (s,a) -> s'\n",
    "    - rTotal[(s,a)][s'] : float\n",
    "        Sum of observed rewards for transitions (s,a) -> s'\n",
    "    - pi : dict\n",
    "        Current greedy policy computed by running value iteration on the\n",
    "        estimated model.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - On each observe/incorporateFeedback call the algorithm updates `tCounts`\n",
    "      and `rTotal`.\n",
    "    - Every `calcValIterEvery` interactions it estimates succAndRewardProb\n",
    "      from counts and runs valueIteration to update pi.\n",
    "\n",
    "    Note: this is a didactic example — real-world model-based agents often\n",
    "    either use Bayesian updates or plan using more sophisticated methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: List[ActionT], discount: float, calcValIterEvery: int = 10000,\n",
    "                 explorationProb: float = 0.2,) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the model-based Monte Carlo learner.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : list\n",
    "            Set of available actions for the MDP.\n",
    "        discount : float\n",
    "            Discount factor for planning (value iteration).\n",
    "        calcValIterEvery : int\n",
    "            How often (in interactions) to estimate the model and run value\n",
    "            iteration to update the policy.\n",
    "        explorationProb : float\n",
    "            Base epsilon for epsilon-greedy exploration during training.\n",
    "        \"\"\"\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.calcValIterEvery = calcValIterEvery\n",
    "        self.explorationProb = explorationProb\n",
    "        self.numIters = 0\n",
    "\n",
    "        # Counts and reward totals used to form an empirical model\n",
    "        self.tCounts = defaultdict(lambda: defaultdict(int))\n",
    "        self.rTotal = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "        # Policy computed by planning on the empirical model\n",
    "        self.pi = {}\n",
    "\n",
    "    def getAction(self, state: StateT, explore: bool = True) -> ActionT:\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection using current policy self.pi.\n",
    "\n",
    "        If `explore` is False, the method will always return the greedy action\n",
    "        from self.pi if available. Otherwise, with probability explorationProb\n",
    "        it will sample a random action from self.actions.\n",
    "        \"\"\"\n",
    "        if explore:\n",
    "            self.numIters += 1\n",
    "        explorationProb = self.explorationProb\n",
    "\n",
    "        # Simple exploration schedule: force full exploration for an initial\n",
    "        # period, then gradually reduce exploration at a slow, logarithmic rate.\n",
    "        if self.numIters < 2e4:  # Always explore early on\n",
    "            explorationProb = 1.0\n",
    "        elif self.numIters > 1e6:  # Decay exploration slowly later\n",
    "            explorationProb = explorationProb / math.log(self.numIters - 100000 + 1)\n",
    "\n",
    "        if not explore or random.random() > explorationProb:\n",
    "            # Exploit: follow the planned policy when available, otherwise pick a random action\n",
    "            return self.pi.get(state, random.choice(self.actions))\n",
    "        else:\n",
    "            # Explore uniformly at random\n",
    "            return random.choice(self.actions)\n",
    "\n",
    "    def incorporateFeedback(self, state: StateT, action: ActionT, reward: int, nextState: StateT, terminal: bool):\n",
    "        \"\"\"\n",
    "        Incorporate a single observed transition into the empirical model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state, action, reward, nextState : transition observation\n",
    "        terminal : bool\n",
    "            True if nextState is terminal.\n",
    "        \"\"\"\n",
    "        # Update counts and reward sums\n",
    "        self.tCounts[(state, action)][nextState] += 1\n",
    "        self.rTotal[(state, action)][nextState] += reward\n",
    "\n",
    "        # Periodically estimate model probabilities and run value iteration\n",
    "        if self.numIters % self.calcValIterEvery == 0:\n",
    "            succAndRewardProb = defaultdict(list)\n",
    "            for (s, a), next_states in self.tCounts.items():\n",
    "                total_count = sum(next_states.values())\n",
    "                if total_count > 0:  # Only include (s,a) pairs we have observed\n",
    "                    for ns, count in next_states.items():\n",
    "                        prob = count / total_count\n",
    "                        avg_reward = self.rTotal[(s, a)][ns] / count\n",
    "                        succAndRewardProb[(s, a)].append((ns, prob, avg_reward))\n",
    "\n",
    "            # Run value iteration on empirical model and update policy\n",
    "            self.pi = valueIteration(succAndRewardProb, self.discount)\n",
    "\n",
    "############################################################\n",
    "# ### Problem 4a: Tabular Q-Learning\n",
    "# Classic Q-learning on finite state-action spaces.\n",
    "############################################################\n",
    "\n",
    "class TabularQLearning(util.RLAlgorithm):\n",
    "    \"\"\"\n",
    "    Tabular Q-learning implementation.\n",
    "\n",
    "    Storage\n",
    "    -------\n",
    "    - self.Q[(state, action)] stores the current Q-value estimate (default initialQ)\n",
    "\n",
    "    Algorithm\n",
    "    ---------\n",
    "    - getAction implements an epsilon-greedy policy with a simple schedule\n",
    "      for exploration.\n",
    "    - incorporateFeedback implements the standard Q-learning update rule:\n",
    "          Q(s,a) <- (1 - alpha) * Q(s,a) + alpha * target\n",
    "      where target = r + gamma * max_a' Q(s',a') (or r if s' is terminal).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions: List[ActionT], discount: float,\n",
    "                 explorationProb: float = 0.2, initialQ: float = 0):\n",
    "        \"\"\"\n",
    "        Initialize the tabular Q-learning agent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : list\n",
    "            The list of actions the agent may choose from.\n",
    "        discount : float\n",
    "            Discount factor gamma.\n",
    "        explorationProb : float\n",
    "            Base epsilon for epsilon-greedy exploration.\n",
    "        initialQ : float\n",
    "            Initial Q-value for unseen (state,action) pairs.\n",
    "        \"\"\"\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.explorationProb = explorationProb\n",
    "        self.Q = defaultdict(lambda: initialQ)\n",
    "        self.numIters = 0\n",
    "\n",
    "    def getAction(self, state: StateT, explore: bool = True) -> ActionT:\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection. If explore=False, always selects the\n",
    "        greedy action (tie-broken by action ordering in max()).\n",
    "        \"\"\"\n",
    "        if explore:\n",
    "            self.numIters += 1\n",
    "        explorationProb = self.explorationProb\n",
    "\n",
    "        if self.numIters < 2e4:  # Strong initial exploration\n",
    "            explorationProb = 1.0\n",
    "        elif self.numIters > 1e5:  # Decay exploration slowly later\n",
    "            explorationProb = explorationProb / math.log(self.numIters - 100000 + 1)\n",
    "\n",
    "        if not explore or random.random() > explorationProb:\n",
    "            # Exploit: choose action with highest Q-value\n",
    "            return max(self.actions, key=lambda a: self.Q[state, a])\n",
    "        else:\n",
    "            # Explore uniformly at random\n",
    "            return random.choice(self.actions)\n",
    "\n",
    "    def getStepSize(self) -> float:\n",
    "        \"\"\"Return the learning rate (alpha) used for Q updates.\"\"\"\n",
    "        return 0.1\n",
    "\n",
    "    def incorporateFeedback(self, state: StateT, action: ActionT, reward: float, nextState: StateT, terminal: bool) -> None:\n",
    "        \"\"\"\n",
    "        Update Q-value for a single transition using Q-learning update.\n",
    "\n",
    "        If `terminal` is True, the target is simply the observed reward.\n",
    "        \"\"\"\n",
    "        step_size = self.getStepSize()\n",
    "\n",
    "        if terminal:\n",
    "            target = reward\n",
    "        else:\n",
    "            # Use the bootstrap estimate of future value: max_{a'} Q(s', a')\n",
    "            next_q_values = [self.Q[nextState, next_action] for next_action in self.actions]\n",
    "            target = reward + self.discount * max(next_q_values)\n",
    "\n",
    "        # Incremental update toward the target\n",
    "        self.Q[state, action] = (1 - step_size) * self.Q[state, action] + step_size * target\n",
    "\n",
    "############################################################\n",
    "# ### Problem 4b: Fourier Feature Extractor\n",
    "# Map continuous states to a fixed-length cosine basis (Fourier basis).\n",
    "############################################################\n",
    "\n",
    "def fourierFeatureExtractor(\n",
    "        state: StateT,\n",
    "        maxCoeff: int = 5,\n",
    "        scale: Optional[Iterable] = None\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute Fourier basis features for a continuous state vector.\n",
    "\n",
    "    For a d-dimensional state, we enumerate all coefficient vectors c in\n",
    "    {0,1,...,maxCoeff}^d and compute features: cos(pi * c . (scale * state)).\n",
    "    The total number of features returned is (maxCoeff + 1)^d.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : sequence-like\n",
    "        Continuous state, e.g. (position, velocity).\n",
    "    maxCoeff : int\n",
    "        Maximum coefficient used per dimension (inclusive).\n",
    "    scale : optional sequence\n",
    "        Multiplicative scaling applied to each state dimension before the\n",
    "        dot product with coefficients. Helpful to normalize ranges of\n",
    "        heterogeneous dimensions (e.g. multiply velocities by a factor).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : np.ndarray\n",
    "        1D array of length (maxCoeff+1)^d containing the Fourier features.\n",
    "    \"\"\"\n",
    "    if scale is None:\n",
    "        scale = np.ones_like(state)\n",
    "    features = None\n",
    "\n",
    "    # Construct the grid of coefficients for each dimension: 0..maxCoeff\n",
    "    dims = [np.arange(maxCoeff + 1) for _ in range(len(state))]\n",
    "\n",
    "    # meshgrid produces a grid for all combinations; then we flatten each\n",
    "    # coordinate grid and stack them as rows of coefficient vectors.\n",
    "    coeffs = np.meshgrid(*dims)\n",
    "    coeffs = np.vstack([c.ravel() for c in coeffs]).T\n",
    "\n",
    "    # Apply scaling to the state and compute dot products (coeffs @ scaled_state)\n",
    "    scaled_state = np.array(state) * np.array(scale)\n",
    "    features = np.cos(np.pi * np.dot(coeffs, scaled_state))\n",
    "\n",
    "    return features\n",
    "\n",
    "############################################################\n",
    "# ### Problem 4c: Function Approximation Q-learning\n",
    "# Q-learning where Q(s,a) is approximated as linear in features: Q = phi(s)^T w_a\n",
    "############################################################\n",
    "\n",
    "class FunctionApproxQLearning(util.RLAlgorithm):\n",
    "    \"\"\"\n",
    "    Linear function-approximation Q-learning.\n",
    "\n",
    "    Model: Q(s,a) = phi(s)^T * W[:, a], where W is a (featureDim x numActions)\n",
    "    weight matrix and phi(s) is the feature vector returned by featureExtractor.\n",
    "\n",
    "    Learning: semi-gradient one-step Q-learning update of the weights for the\n",
    "    selected action column.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, featureDim: int, featureExtractor: Callable, actions: List[int],\n",
    "                 discount: float, explorationProb=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the function-approx Q learner.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        featureDim : int\n",
    "            Dimension of the feature vector returned by featureExtractor.\n",
    "        featureExtractor : callable\n",
    "            Function mapping state -> numpy array of length featureDim.\n",
    "        actions : list[int]\n",
    "            Action indices supported by the agent.\n",
    "        discount : float\n",
    "            Discount factor gamma.\n",
    "        explorationProb : float\n",
    "            Base epsilon for epsilon-greedy exploration.\n",
    "        \"\"\"\n",
    "        self.featureDim = featureDim\n",
    "        self.featureExtractor = featureExtractor\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.explorationProb = explorationProb\n",
    "        # Initialize weights randomly to break symmetry; shape: (featureDim, numActions)\n",
    "        self.W = np.random.standard_normal(size=(featureDim, len(actions)))\n",
    "        self.numIters = 0\n",
    "\n",
    "    def getQ(self, state: np.ndarray, action: int) -> float:\n",
    "        \"\"\"Return approximate Q(s,a) = phi(s)^T w_a.\"\"\"\n",
    "        features = self.featureExtractor(state)\n",
    "        return float(features.dot(self.W[:, action]))\n",
    "\n",
    "    def getAction(self, state: np.ndarray, explore: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection using approximated Q-values.\n",
    "\n",
    "        Behavior mirrors the TabularQLearning.getAction implementation.\n",
    "        \"\"\"\n",
    "        if explore:\n",
    "            self.numIters += 1\n",
    "        explorationProb = self.explorationProb\n",
    "        if self.numIters < 2e4:  # force initial exploration\n",
    "            explorationProb = 1.0\n",
    "        elif self.numIters > 1e5:  # slow logarithmic decay later\n",
    "            explorationProb = explorationProb / math.log(self.numIters - 100000 + 1)\n",
    "\n",
    "        if not explore or random.random() > explorationProb:\n",
    "            # Exploit: choose action with highest approximated Q-value\n",
    "            return int(max(range(len(self.actions)), key=lambda a: self.getQ(state, a)))\n",
    "        else:\n",
    "            # Explore uniformly at random\n",
    "            return random.randrange(len(self.actions))\n",
    "\n",
    "    def getStepSize(self) -> float:\n",
    "        \"\"\"Return a decaying step size (learning rate) used for weight updates.\"\"\"\n",
    "        return 0.005 * (0.99)**(self.numIters / 500)\n",
    "\n",
    "    def incorporateFeedback(self, state: np.ndarray, action: int, reward: float, nextState: np.ndarray, terminal: bool) -> None:\n",
    "        \"\"\"\n",
    "        Perform a semi-gradient Q-learning update on the weights corresponding\n",
    "        to `action` using the observed transition (s,a,r,s').\n",
    "\n",
    "        If `terminal` is True, the target is just `reward`, otherwise target =\n",
    "        reward + discount * max_a' Q(nextState, a').\n",
    "        \"\"\"\n",
    "        features = self.featureExtractor(state)\n",
    "\n",
    "        if terminal:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q_values = [self.getQ(nextState, a) for a in range(len(self.actions))]\n",
    "            target = reward + self.discount * max(next_q_values)\n",
    "\n",
    "        prediction = self.getQ(state, action)\n",
    "        step_size = self.getStepSize()\n",
    "\n",
    "        # Update only the column of W corresponding to the selected action\n",
    "        # using the semi-gradient TD error (target - prediction) times features.\n",
    "        self.W[:, action] += step_size * (target - prediction) * features\n",
    "\n",
    "############################################################\n",
    "# ### Problem 5c: Constrained Q-learning\n",
    "# Extend function-approx Q-learning to enforce velocity constraints in actions\n",
    "############################################################\n",
    "\n",
    "class ConstrainedQLearning(FunctionApproxQLearning):\n",
    "    \"\"\"\n",
    "    Constrained Q-learning that disallows actions that would lead to a next\n",
    "    velocity exceeding `max_speed` (approximate one-step check using the known\n",
    "    physics constants `force` and `gravity`).\n",
    "\n",
    "    This class reuses the function approximation update rule but overrides\n",
    "    getAction to only consider valid actions under the constraint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, featureDim: int, featureExtractor: Callable, actions: List[int],\n",
    "                 discount: float, force: float, gravity: float,\n",
    "                 max_speed: Optional[float] = None,\n",
    "                 explorationProb=0.2):\n",
    "        \"\"\"\n",
    "        Initialize constrained Q-learner.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        featureDim, featureExtractor, actions, discount, explorationProb\n",
    "            See parent class.\n",
    "        force : float\n",
    "            Magnitude of the control force used in the mountain car dynamics\n",
    "            (used to approximate next velocity when checking validity).\n",
    "        gravity : float\n",
    "            Gravity constant used in the dynamics model.\n",
    "        max_speed : float or None\n",
    "            If not None, the maximum allowed absolute velocity after an action.\n",
    "        \"\"\"\n",
    "        super().__init__(featureDim, featureExtractor, actions,\n",
    "                         discount, explorationProb)\n",
    "        self.force = force\n",
    "        self.gravity = gravity\n",
    "        self.max_speed = max_speed\n",
    "\n",
    "    def getAction(self, state: np.ndarray, explore: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Choose an action among those that are valid (do not violate the\n",
    "        approximate max-speed constraint). Behavior otherwise mirrors\n",
    "        epsilon-greedy selection from the parent class.\n",
    "        \"\"\"\n",
    "        if explore:\n",
    "            self.numIters += 1\n",
    "        explorationProb = self.explorationProb\n",
    "        if self.numIters < 2e4:\n",
    "            explorationProb = 1.0\n",
    "        elif self.numIters > 1e5:\n",
    "            explorationProb = explorationProb / math.log(self.numIters - 100000 + 1)\n",
    "\n",
    "        def is_valid_action(action, current_velocity):\n",
    "            \"\"\"Approximate one-step dynamics to decide if action is valid.\n",
    "\n",
    "            Uses the mountain car dynamics simplification:\n",
    "                v' = v + applied_force * self.force - cos(3 * position) * self.gravity\n",
    "            where applied_force is in {-1, 0, +1} depending on action index.\n",
    "            \"\"\"\n",
    "            velocity = current_velocity\n",
    "            position = state[0]\n",
    "            if action == 0: force = -1.0\n",
    "            elif action == 2: force = 1.0\n",
    "            else: force = 0\n",
    "\n",
    "            next_velocity = velocity + force * self.force - math.cos(3 * position) * self.gravity\n",
    "            if self.max_speed is not None and abs(next_velocity) > self.max_speed:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        # Filter actions to those that pass the approximate constraint check\n",
    "        valid_actions = [a for a in range(len(self.actions)) if is_valid_action(a, state[1])]\n",
    "\n",
    "        if not valid_actions:\n",
    "            # Rare fallback: choose neutral action (index 1) if nothing valid\n",
    "            return 1\n",
    "\n",
    "        if not explore or random.random() > explorationProb:\n",
    "            # Greedy among valid actions\n",
    "            return max(valid_actions, key=lambda a: self.getQ(state, a))\n",
    "        else:\n",
    "            # Random exploration among valid actions\n",
    "            return random.choice(valid_actions)\n",
    "\n",
    "############################################################\n",
    "# ### Helper code: Gym registration and evaluation utilities\n",
    "############################################################\n",
    "\n",
    "# Register the custom MountainCar environment so it can be created by gym.make\n",
    "gym.register(\n",
    "    id=\"CustomMountainCar-v0\",\n",
    "    entry_point=\"custom_mountain_car:CustomMountainCarEnv\",\n",
    "    max_episode_steps=1000,\n",
    "    reward_threshold=-110.0,\n",
    ")\n",
    "\n",
    "# Instantiate two MDP wrappers for experiments and comparison\n",
    "mdp1 = ContinuousGymMDP(\"CustomMountainCar-v0\", discount=0.999, timeLimit=1000)\n",
    "mdp2 = ContinuousGymMDP(\"CustomMountainCar-v0\", discount=0.999, timeLimit=1000)\n",
    "\n",
    "\n",
    "def compare_MDP_Strategies(mdp1: ContinuousGymMDP, mdp2: ContinuousGymMDP):\n",
    "    \"\"\"\n",
    "    Example experiment for Problem 5c: train two constrained learners (one\n",
    "    with large max_speed and one with a strict max_speed) and sample\n",
    "    trajectories to compare the frequency of actions chosen.\n",
    "\n",
    "    This function demonstrates how to instantiate ConstrainedQLearning and\n",
    "    how to call the helper `sampleKRLTrajectories` defined below.\n",
    "    \"\"\"\n",
    "    rl1 = ConstrainedQLearning(\n",
    "        36,\n",
    "        lambda s: fourierFeatureExtractor(s, maxCoeff=5, scale=[1, 15]),\n",
    "        mdp1.actions,\n",
    "        mdp1.discount,\n",
    "        mdp1.env.force,\n",
    "        mdp1.env.gravity,\n",
    "        10000,\n",
    "        explorationProb=0.2,\n",
    "    )\n",
    "    rl2 = ConstrainedQLearning(\n",
    "        36,\n",
    "        lambda s: fourierFeatureExtractor(s, maxCoeff=5, scale=[1, 15]),\n",
    "        mdp2.actions,\n",
    "        mdp2.discount,\n",
    "        mdp2.env.force,\n",
    "        mdp2.env.gravity,\n",
    "        0.065,\n",
    "        explorationProb=0.2,\n",
    "    )\n",
    "    sampleKRLTrajectories(mdp1, rl1)\n",
    "    sampleKRLTrajectories(mdp2, rl2)\n",
    "\n",
    "\n",
    "def sampleKRLTrajectories(mdp: ContinuousGymMDP, rl: ConstrainedQLearning):\n",
    "    \"\"\"\n",
    "    Sample a number of trajectories using the provided RL agent and summarize\n",
    "    the action counts. This relies on the helper util.sample_RL_trajectory\n",
    "    function (provided by the assignment utilities).\n",
    "    \"\"\"\n",
    "    accelerate_left, no_accelerate, accelerate_right = 0, 0, 0\n",
    "    for n in range(100):\n",
    "        traj = util.sample_RL_trajectory(mdp, rl)\n",
    "        accelerate_left = traj.count(0)\n",
    "        no_accelerate = traj.count(1)\n",
    "        accelerate_right = traj.count(2)\n",
    "\n",
    "    print(f\"\\nRL with MDP -> start state:{mdp.startState()}, max_speed:{rl.max_speed}\")\n",
    "    print(f\"  *  total accelerate left actions: {accelerate_left}, total no acceleration actions: {no_accelerate}, total accelerate right actions: {accelerate_right}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN BLOCK - Comprehensive Testing and Demonstrations\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 4: MOUNTAIN CAR REINFORCEMENT LEARNING - DEMOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Demo 1: Value Iteration on Number Line MDP\n",
    "    print(\"\\n--- Demo 1: Value Iteration on Number Line MDP ---\")\n",
    "    try:\n",
    "        numberLineMDP = util.NumberLineMDP(n=5)\n",
    "        print(f\"Created NumberLine MDP with n={numberLineMDP.n}\")\n",
    "        print(f\"States: from -{numberLineMDP.n} to {numberLineMDP.n}\")\n",
    "        print(f\"Actions: {numberLineMDP.actions}\")\n",
    "        print(f\"Discount: {numberLineMDP.discount}\")\n",
    "        print(f\"Left reward: {numberLineMDP.leftReward}, Right reward: {numberLineMDP.rightReward}\")\n",
    "        print(f\"Penalty: {numberLineMDP.penalty}\")\n",
    "        \n",
    "        print(\"\\nRunning Value Iteration...\")\n",
    "        policy = run_VI_over_numberLine(numberLineMDP)\n",
    "        print(f\"✓ Optimal policy computed\")\n",
    "        print(\"Sample policy (state -> action):\")\n",
    "        for state in sorted(list(policy.keys())[:10]):\n",
    "            print(f\"  state {state:3d} -> action {policy[state]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 2: Fourier Feature Extraction\n",
    "    print(\"\\n--- Demo 2: Fourier Feature Extraction ---\")\n",
    "    try:\n",
    "        # Test Fourier features with different states\n",
    "        test_states = [\n",
    "            np.array([0.0, 0.0]),\n",
    "            np.array([0.5, 0.02]),\n",
    "            np.array([-0.5, -0.02]),\n",
    "            np.array([0.45, 0.05])\n",
    "        ]\n",
    "        \n",
    "        for state in test_states:\n",
    "            features = fourierFeatureExtractor(state, maxCoeff=3, scale=[1, 10])\n",
    "            print(f\"State {state} -> {len(features)} features\")\n",
    "            print(f\"  First 10 features: {features[:10]}\")\n",
    "            print(f\"  Feature norm: {np.linalg.norm(features):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 3: Tabular Q-Learning (Small Example)\n",
    "    print(\"\\n--- Demo 3: Tabular Q-Learning (Conceptual Demo) ---\")\n",
    "    try:\n",
    "        print(\"Tabular Q-Learning uses a table to store Q(s,a) for discrete states.\")\n",
    "        print(\"For Mountain Car with continuous states, we use function approximation instead.\")\n",
    "        print(\"See Demo 4 for Function Approximation Q-Learning.\")\n",
    "        \n",
    "        # Create a simple demonstration with abstract state\n",
    "        tabularRL = TabularQLearning(\n",
    "            actions=[0, 1, 2],  # Mountain car actions\n",
    "            discount=0.99,\n",
    "            explorationProb=0.2\n",
    "        )\n",
    "        print(f\"✓ Created Tabular Q-Learning agent\")\n",
    "        print(f\"  Actions: {tabularRL.actions}\")\n",
    "        print(f\"  Discount: {tabularRL.discount}\")\n",
    "        print(f\"  Exploration prob: {tabularRL.explorationProb}\")\n",
    "        \n",
    "        # Simulate a few updates\n",
    "        dummy_state = \"state_1\"\n",
    "        action = 1\n",
    "        reward = -1.0\n",
    "        next_state = \"state_2\"\n",
    "        \n",
    "        print(f\"\\nSimulating Q-learning update:\")\n",
    "        print(f\"  Before: Q({dummy_state}, {action}) = {tabularRL.Q[dummy_state, action]}\")\n",
    "        tabularRL.incorporateFeedback(dummy_state, action, reward, next_state, False)\n",
    "        print(f\"  After: Q({dummy_state}, {action}) = {tabularRL.Q[dummy_state, action]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 4: Function Approximation Q-Learning\n",
    "    print(\"\\n--- Demo 4: Function Approximation Q-Learning ---\")\n",
    "    try:\n",
    "        print(\"Setting up Function Approximation Q-Learning for Mountain Car...\")\n",
    "        \n",
    "        featureDim = 36  # (maxCoeff+1)^2 = (5+1)^2 = 36\n",
    "        featureExtractor = lambda s: fourierFeatureExtractor(s, maxCoeff=5, scale=[1, 15])\n",
    "        actions = [0, 1, 2]\n",
    "        discount = 0.999\n",
    "        \n",
    "        funcApproxRL = FunctionApproxQLearning(\n",
    "            featureDim=featureDim,\n",
    "            featureExtractor=featureExtractor,\n",
    "            actions=actions,\n",
    "            discount=discount,\n",
    "            explorationProb=0.2\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Created Function Approximation Q-Learning agent\")\n",
    "        print(f\"  Feature dimension: {featureDim}\")\n",
    "        print(f\"  Actions: {actions}\")\n",
    "        print(f\"  Weight matrix shape: {funcApproxRL.W.shape}\")\n",
    "        print(f\"  Discount: {discount}\")\n",
    "        \n",
    "        # Test on sample states\n",
    "        sample_state = np.array([0.0, 0.0])\n",
    "        print(f\"\\nTesting on sample state {sample_state}:\")\n",
    "        for action in actions:\n",
    "            q_value = funcApproxRL.getQ(sample_state, action)\n",
    "            print(f\"  Q({sample_state}, action={action}) = {q_value:.6f}\")\n",
    "        \n",
    "        best_action = funcApproxRL.getAction(sample_state, explore=False)\n",
    "        print(f\"  Best action (greedy): {best_action}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 5: Constrained Q-Learning\n",
    "    print(\"\\n--- Demo 5: Constrained Q-Learning ---\")\n",
    "    try:\n",
    "        print(\"Setting up Constrained Q-Learning with velocity constraints...\")\n",
    "        \n",
    "        # Create constrained learner with max_speed limit\n",
    "        constrained_rl = ConstrainedQLearning(\n",
    "            featureDim=36,\n",
    "            featureExtractor=lambda s: fourierFeatureExtractor(s, maxCoeff=5, scale=[1, 15]),\n",
    "            actions=[0, 1, 2],\n",
    "            discount=0.999,\n",
    "            force=0.001,  # Mountain car force constant\n",
    "            gravity=0.0025,  # Mountain car gravity constant\n",
    "            max_speed=0.07,  # Velocity constraint\n",
    "            explorationProb=0.2\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Created Constrained Q-Learning agent\")\n",
    "        print(f\"  Max speed constraint: {constrained_rl.max_speed}\")\n",
    "        print(f\"  Force: {constrained_rl.force}\")\n",
    "        print(f\"  Gravity: {constrained_rl.gravity}\")\n",
    "        \n",
    "        # Test constraint validation\n",
    "        test_states_constrained = [\n",
    "            np.array([0.0, 0.0]),    # Low velocity - all actions valid\n",
    "            np.array([0.0, 0.06]),   # High velocity - some actions invalid\n",
    "            np.array([0.5, -0.06]),  # High negative velocity\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting action validity under constraints:\")\n",
    "        for state in test_states_constrained:\n",
    "            action = constrained_rl.getAction(state, explore=False)\n",
    "            print(f\"  State {state} -> chosen action: {action}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 5: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 6: MDP Comparison (Conceptual)\n",
    "    print(\"\\n--- Demo 6: MDP Strategy Comparison (Conceptual) ---\")\n",
    "    print(\"The compare_MDP_Strategies function trains two agents with different\")\n",
    "    print(\"max_speed constraints and compares their action distributions.\")\n",
    "    print(\"This requires full training loops which take time, so we skip actual\")\n",
    "    print(\"training here, but the framework is ready in compare_MDP_Strategies().\")\n",
    "    print(\"\\nTo run full training experiments, call:\")\n",
    "    print(\"  compare_MDP_Strategies(mdp1, mdp2)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ASSIGNMENT 4: ALL DEMOS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nNote: Full RL training requires many episodes and is computationally\")\n",
    "    print(\"expensive. The demos above show the components are correctly implemented.\")\n",
    "    print(\"For full training, use the provided train.py script:\")\n",
    "    print(\"  python mountaincar/train.py --agent tabular\")\n",
    "    print(\"  python mountaincar/train.py --agent function_approx\")\n",
    "    print(\"  python mountaincar/train.py --agent constrained\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# END OF FILE\n",
    "# ============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f79c2",
   "metadata": {},
   "source": [
    "## Mountain Car Main Block\n",
    "\n",
    "This section demonstrates all the key components implemented in Assignment 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN BLOCK - Comprehensive Testing and Demonstrations\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 4: MOUNTAIN CAR REINFORCEMENT LEARNING - DEMOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Demo 1: Value Iteration on Number Line MDP\n",
    "    print(\"\\n--- Demo 1: Value Iteration on Number Line MDP ---\")\n",
    "    try:\n",
    "        numberLineMDP = util.NumberLineMDP(n=5)\n",
    "        print(f\"Created NumberLine MDP with n={numberLineMDP.n}\")\n",
    "        print(f\"States: from -{numberLineMDP.n} to {numberLineMDP.n}\")\n",
    "        print(f\"Actions: {numberLineMDP.actions}\")\n",
    "        print(f\"Discount: {numberLineMDP.discount}\")\n",
    "        print(f\"Left reward: {numberLineMDP.leftReward}, Right reward: {numberLineMDP.rightReward}\")\n",
    "        print(f\"Penalty: {numberLineMDP.penalty}\")\n",
    "        \n",
    "        print(\"\\nRunning Value Iteration...\")\n",
    "        policy = run_VI_over_numberLine(numberLineMDP)\n",
    "        print(f\"✓ Optimal policy computed\")\n",
    "        print(\"Sample policy (state -> action):\")\n",
    "        for state in sorted(list(policy.keys())[:10]):\n",
    "            print(f\"  state {state:3d} -> action {policy[state]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 2: Fourier Feature Extraction\n",
    "    print(\"\\n--- Demo 2: Fourier Feature Extraction ---\")\n",
    "    try:\n",
    "        # Test Fourier features with different states\n",
    "        test_states = [\n",
    "            np.array([0.0, 0.0]),\n",
    "            np.array([0.5, 0.02]),\n",
    "            np.array([-0.5, -0.02]),\n",
    "            np.array([0.45, 0.05])\n",
    "        ]\n",
    "        \n",
    "        for state in test_states:\n",
    "            features = fourierFeatureExtractor(state, maxCoeff=3, scale=[1, 10])\n",
    "            print(f\"State {state} -> {len(features)} features\")\n",
    "            print(f\"  First 10 features: {features[:10]}\")\n",
    "            print(f\"  Feature norm: {np.linalg.norm(features):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 3: Tabular Q-Learning (Small Example)\n",
    "    print(\"\\n--- Demo 3: Tabular Q-Learning (Conceptual Demo) ---\")\n",
    "    try:\n",
    "        print(\"Tabular Q-Learning uses a table to store Q(s,a) for discrete states.\")\n",
    "        print(\"For Mountain Car with continuous states, we use function approximation instead.\")\n",
    "        print(\"See Demo 4 for Function Approximation Q-Learning.\")\n",
    "        \n",
    "        # Create a simple demonstration with abstract state\n",
    "        tabularRL = TabularQLearning(\n",
    "            actions=[0, 1, 2],  # Mountain car actions\n",
    "            discount=0.99,\n",
    "            explorationProb=0.2\n",
    "        )\n",
    "        print(f\"✓ Created Tabular Q-Learning agent\")\n",
    "        print(f\"  Actions: {tabularRL.actions}\")\n",
    "        print(f\"  Discount: {tabularRL.discount}\")\n",
    "        print(f\"  Exploration prob: {tabularRL.explorationProb}\")\n",
    "        \n",
    "        # Simulate a few updates\n",
    "        dummy_state = \"state_1\"\n",
    "        action = 1\n",
    "        reward = -1.0\n",
    "        next_state = \"state_2\"\n",
    "        \n",
    "        print(f\"\\nSimulating Q-learning update:\")\n",
    "        print(f\"  Before: Q({dummy_state}, {action}) = {tabularRL.Q[dummy_state, action]}\")\n",
    "        tabularRL.incorporateFeedback(dummy_state, action, reward, next_state, False)\n",
    "        print(f\"  After: Q({dummy_state}, {action}) = {tabularRL.Q[dummy_state, action]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 4: Function Approximation Q-Learning\n",
    "    print(\"\\n--- Demo 4: Function Approximation Q-Learning ---\")\n",
    "    try:\n",
    "        print(\"Setting up Function Approximation Q-Learning for Mountain Car...\")\n",
    "        \n",
    "        featureDim = 36  # (maxCoeff+1)^2 = (5+1)^2 = 36\n",
    "        featureExtractor = lambda s: fourierFeatureExtractor(s, maxCoeff=5, scale=[1, 15])\n",
    "        actions = [0, 1, 2]\n",
    "        discount = 0.999\n",
    "        \n",
    "        funcApproxRL = FunctionApproxQLearning(\n",
    "            featureDim=featureDim,\n",
    "            featureExtractor=featureExtractor,\n",
    "            actions=actions,\n",
    "            discount=discount,\n",
    "            explorationProb=0.2\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Created Function Approximation Q-Learning agent\")\n",
    "        print(f\"  Feature dimension: {featureDim}\")\n",
    "        print(f\"  Actions: {actions}\")\n",
    "        print(f\"  Weight matrix shape: {funcApproxRL.W.shape}\")\n",
    "        print(f\"  Discount: {discount}\")\n",
    "        \n",
    "        # Test on sample states\n",
    "        sample_state = np.array([0.0, 0.0])\n",
    "        print(f\"\\nTesting on sample state {sample_state}:\")\n",
    "        for action in actions:\n",
    "            q_value = funcApproxRL.getQ(sample_state, action)\n",
    "            print(f\"  Q({sample_state}, action={action}) = {q_value:.6f}\")\n",
    "        \n",
    "        best_action = funcApproxRL.getAction(sample_state, explore=False)\n",
    "        print(f\"  Best action (greedy): {best_action}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 5: Constrained Q-Learning\n",
    "    print(\"\\n--- Demo 5: Constrained Q-Learning ---\")\n",
    "    try:\n",
    "        print(\"Setting up Constrained Q-Learning with velocity constraints...\")\n",
    "        \n",
    "        # Create constrained learner with max_speed limit\n",
    "        constrained_rl = ConstrainedQLearning(\n",
    "            featureDim=36,\n",
    "            featureExtractor=lambda s: fourierFeatureExtractor(s, maxCoeff=5, scale=[1, 15]),\n",
    "            actions=[0, 1, 2],\n",
    "            discount=0.999,\n",
    "            force=0.001,  # Mountain car force constant\n",
    "            gravity=0.0025,  # Mountain car gravity constant\n",
    "            max_speed=0.07,  # Velocity constraint\n",
    "            explorationProb=0.2\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Created Constrained Q-Learning agent\")\n",
    "        print(f\"  Max speed constraint: {constrained_rl.max_speed}\")\n",
    "        print(f\"  Force: {constrained_rl.force}\")\n",
    "        print(f\"  Gravity: {constrained_rl.gravity}\")\n",
    "        \n",
    "        # Test constraint validation\n",
    "        test_states_constrained = [\n",
    "            np.array([0.0, 0.0]),    # Low velocity - all actions valid\n",
    "            np.array([0.0, 0.06]),   # High velocity - some actions invalid\n",
    "            np.array([0.5, -0.06]),  # High negative velocity\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting action validity under constraints:\")\n",
    "        for state in test_states_constrained:\n",
    "            action = constrained_rl.getAction(state, explore=False)\n",
    "            print(f\"  State {state} -> chosen action: {action}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 5: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 6: MDP Comparison (Conceptual)\n",
    "    print(\"\\n--- Demo 6: MDP Strategy Comparison (Conceptual) ---\")\n",
    "    print(\"The compare_MDP_Strategies function trains two agents with different\")\n",
    "    print(\"max_speed constraints and compares their action distributions.\")\n",
    "    print(\"This requires full training loops which take time, so we skip actual\")\n",
    "    print(\"training here, but the framework is ready in compare_MDP_Strategies().\")\n",
    "    print(\"\\nTo run full training experiments, call:\")\n",
    "    print(\"  compare_MDP_Strategies(mdp1, mdp2)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ASSIGNMENT 4: ALL DEMOS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nNote: Full RL training requires many episodes and is computationally\")\n",
    "    print(\"expensive. The demos above show the components are correctly implemented.\")\n",
    "    print(\"For full training, use the provided train.py script:\")\n",
    "    print(\"  python mountaincar/train.py --agent tabular\")\n",
    "    print(\"  python mountaincar/train.py --agent function_approx\")\n",
    "    print(\"  python mountaincar/train.py --agent constrained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e6f48-39d8-4ff6-9d86-63ef53bb789a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 5\n",
    "# Title : Pac-Man Multi-Agent Search\n",
    "\n",
    "**Description:**  \n",
    "This assignment implements multi-agent search algorithms for the Pac-Man game.  \n",
    "It includes the following key tasks:  \n",
    "- Reflex Agent (baseline)  \n",
    "- Minimax Search  \n",
    "- Alpha-Beta Pruning (TODO)  \n",
    "- Expectimax Search (TODO)  \n",
    "- Advanced Evaluation Function (Better Eval, TODO)  \n",
    "\n",
    "Each agent is implemented with detailed docstrings and inline comments for clarity.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb87ee8-a598-42ee-82c5-755dd2b2a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from util import manhattanDistance\n",
    "from game import Directions\n",
    "import random\n",
    "import util\n",
    "from typing import Any, DefaultDict, List, Set, Tuple\n",
    "\n",
    "from game import Agent\n",
    "from pacman import GameState\n",
    "\n",
    "# ============================================================\n",
    "# ### Problem 1a: Reflex Agent\n",
    "# ============================================================\n",
    "# The ReflexAgent chooses an action at each choice point by examining\n",
    "# its immediate successor states using an evaluation function.\n",
    "\n",
    "class ReflexAgent(Agent):\n",
    "    \"\"\"ReflexAgent chooses actions based on an immediate evaluation of successor states.\n",
    "\n",
    "    At each decision point, the agent looks at all legal actions available to Pac-Man,\n",
    "    computes an evaluation score for each possible resulting successor state, and picks\n",
    "    the one that appears most promising.\n",
    "\n",
    "    Attributes:\n",
    "        lastPositions (list): Keeps track of recent positions of Pac-Man.\n",
    "        dc (Any): Placeholder for potential debugging or additional computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lastPositions = []\n",
    "        self.dc = None\n",
    "\n",
    "    def getAction(self, gameState: GameState):\n",
    "        \"\"\"Chooses among the best options according to the evaluation function.\n",
    "\n",
    "        Args:\n",
    "            gameState (GameState): The current game state object.\n",
    "\n",
    "        Returns:\n",
    "            str: The chosen action direction (e.g., 'North', 'South', etc.).\n",
    "\n",
    "        The function retrieves all legal Pac-Man actions, evaluates each resulting\n",
    "        successor state using the evaluationFunction, and selects the action(s)\n",
    "        that yield the highest score.\n",
    "        \"\"\"\n",
    "        # Collect all legal moves for Pac-Man\n",
    "        legalMoves = gameState.getLegalActions()\n",
    "\n",
    "        # Compute evaluation scores for each possible action\n",
    "        scores = [self.evaluationFunction(gameState, action) for action in legalMoves]\n",
    "        bestScore = max(scores)\n",
    "\n",
    "        # Identify all actions that yield the maximum evaluation score\n",
    "        bestIndices = [index for index in range(len(scores)) if scores[index] == bestScore]\n",
    "\n",
    "        # Randomly choose among the best actions to add some variability\n",
    "        chosenIndex = random.choice(bestIndices)\n",
    "        return legalMoves[chosenIndex]\n",
    "\n",
    "    def evaluationFunction(self, currentGameState: GameState, action: str) -> float:\n",
    "        \"\"\"Estimates the desirability of a successor state for a given action.\n",
    "\n",
    "        Args:\n",
    "            currentGameState (GameState): The current game state.\n",
    "            action (str): The action to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            float: The estimated value of the successor state.\n",
    "\n",
    "        This function generates the successor state that would result from Pac-Man\n",
    "        taking the specified action, then extracts information such as Pac-Man's new\n",
    "        position, remaining food, and ghost states to compute a score.\n",
    "        \"\"\"\n",
    "        # Generate successor state after taking the given action\n",
    "        successorGameState = currentGameState.generatePacmanSuccessor(action)\n",
    "        newPos = successorGameState.getPacmanPosition()\n",
    "        oldFood = currentGameState.getFood()\n",
    "        newGhostStates = successorGameState.getGhostStates()\n",
    "        newScaredTimes = [ghostState.scaredTimer for ghostState in newGhostStates]\n",
    "\n",
    "        # For now, use the game's internal score directly as evaluation\n",
    "        return successorGameState.getScore()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ### Problem 1a Helper: Default Evaluation Function\n",
    "# ============================================================\n",
    "\n",
    "def scoreEvaluationFunction(currentGameState: GameState):\n",
    "    \"\"\"A simple evaluation function that returns the current game score.\n",
    "\n",
    "    Args:\n",
    "        currentGameState (GameState): The state whose score should be returned.\n",
    "\n",
    "    Returns:\n",
    "        float: The state's current score.\n",
    "\n",
    "    This function is used as the default evaluation function for adversarial\n",
    "    search agents like Minimax or Expectimax, as the game score already encodes\n",
    "    performance metrics such as food collected and ghost avoidance.\n",
    "    \"\"\"\n",
    "    return currentGameState.getScore()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ### MultiAgentSearchAgent Base Class\n",
    "# ============================================================\n",
    "# This abstract base class defines shared behavior for all adversarial agents\n",
    "# such as Minimax, Alpha-Beta, and Expectimax.\n",
    "\n",
    "class MultiAgentSearchAgent(Agent):\n",
    "    \"\"\"Abstract base class providing common setup for multi-agent search agents.\n",
    "\n",
    "    Attributes:\n",
    "        index (int): Pac-Man is always agent 0.\n",
    "        evaluationFunction (function): Function used to evaluate terminal or depth-limited states.\n",
    "        depth (int): Number of plies to look ahead in the search tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, evalFn='scoreEvaluationFunction', depth='2'):\n",
    "        self.index = 0  # Pac-Man is always agent index 0\n",
    "        self.evaluationFunction = util.lookup(evalFn, globals())\n",
    "        self.depth = int(depth)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ### Problem 1b: Minimax Agent\n",
    "# ============================================================\n",
    "# Implements the standard Minimax algorithm for adversarial search.\n",
    "# Pac-Man is the maximizing agent (index 0), while ghosts are minimizing agents.\n",
    "\n",
    "class MinimaxAgent(MultiAgentSearchAgent):\n",
    "    \"\"\"Implements the Minimax algorithm for multi-agent adversarial search.\n",
    "\n",
    "    The agent explores possible future game states up to a specified depth,\n",
    "    assuming optimal play by both Pac-Man (maximizing) and ghosts (minimizing).\n",
    "    \"\"\"\n",
    "\n",
    "    def getAction(self, gameState: GameState) -> str:\n",
    "        \"\"\"Returns the Minimax action from the current game state.\n",
    "\n",
    "        Args:\n",
    "            gameState (GameState): The current state of the Pac-Man game.\n",
    "\n",
    "        Returns:\n",
    "            str: The action determined by the Minimax decision rule.\n",
    "        \"\"\"\n",
    "\n",
    "        def minimax(state: GameState, depth: int, agentIndex: int) -> tuple:\n",
    "            \"\"\"Recursive helper implementing Minimax logic.\n",
    "\n",
    "            Args:\n",
    "                state (GameState): The current game state in the search tree.\n",
    "                depth (int): Remaining search depth.\n",
    "                agentIndex (int): The index of the current agent.\n",
    "\n",
    "            Returns:\n",
    "                tuple: (score, action) for this branch of the search tree.\n",
    "            \"\"\"\n",
    "            # Base case: terminal state (win/lose) or no legal actions\n",
    "            if state.isWin() or state.isLose() or not state.getLegalActions(agentIndex):\n",
    "                return state.getScore(), None\n",
    "\n",
    "            # Depth-limited condition: only apply at Pac-Man's turn\n",
    "            if depth == 0 and agentIndex == 0:\n",
    "                return self.evaluationFunction(state), None\n",
    "\n",
    "            actions = state.getLegalActions(agentIndex)\n",
    "            nextAgent = (agentIndex + 1) % state.getNumAgents()\n",
    "\n",
    "            # Reduce depth only after all agents have moved once\n",
    "            nextDepth = depth - 1 if nextAgent == 0 else depth\n",
    "\n",
    "            # Maximize for Pac-Man (agentIndex 0)\n",
    "            if agentIndex == 0:\n",
    "                bestScore = float('-inf')\n",
    "                bestAction = None\n",
    "                for action in actions:\n",
    "                    score, _ = minimax(state.generateSuccessor(agentIndex, action), nextDepth, nextAgent)\n",
    "                    if score > bestScore:\n",
    "                        bestScore = score\n",
    "                        bestAction = action\n",
    "            else:\n",
    "                # Minimize for ghosts\n",
    "                bestScore = float('inf')\n",
    "                bestAction = None\n",
    "                for action in actions:\n",
    "                    score, _ = minimax(state.generateSuccessor(agentIndex, action), nextDepth, nextAgent)\n",
    "                    if score < bestScore:\n",
    "                        bestScore = score\n",
    "                        bestAction = action\n",
    "\n",
    "            return bestScore, bestAction\n",
    "\n",
    "        # Start Minimax recursion from Pac-Man (agent 0)\n",
    "        _, action = minimax(gameState, self.depth, 0)\n",
    "        return action\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ### Problem 2a: Alpha-Beta Pruning Agent (TODO)\n",
    "# ============================================================\n",
    "# This agent will implement the same logic as Minimax but use alpha-beta pruning\n",
    "# to eliminate branches that cannot influence the final decision.\n",
    "\n",
    "class AlphaBetaAgent(MultiAgentSearchAgent):\n",
    "    \"\"\"Implements Minimax with Alpha-Beta pruning (TODO).\n",
    "\n",
    "    You will need to implement pruning logic that maintains and updates alpha and beta\n",
    "    thresholds during search to cut off branches that cannot affect the final outcome.\n",
    "    \"\"\"\n",
    "\n",
    "    def getAction(self, gameState: GameState) -> str:\n",
    "        \"\"\"Returns the best action using Minimax with Alpha-Beta pruning.\n",
    "\n",
    "        Args:\n",
    "            gameState (GameState): The current Pac-Man game state.\n",
    "\n",
    "        Returns:\n",
    "            str: The optimal action based on Alpha-Beta search.\n",
    "        \"\"\"\n",
    "        # TODO: Implement alpha-beta pruning logic here\n",
    "        raise Exception(\"Not implemented yet\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ### Problem 3b: Expectimax Agent (TODO)\n",
    "# ============================================================\n",
    "# This agent models ghost actions as stochastic — ghosts choose randomly from legal actions.\n",
    "\n",
    "class ExpectimaxAgent(MultiAgentSearchAgent):\n",
    "    \"\"\"Implements the Expectimax algorithm (TODO).\n",
    "\n",
    "    Unlike Minimax, ghosts are modeled as choosing uniformly at random among legal actions.\n",
    "    The value of a state under an expectimax tree is therefore the expected value, not the min.\n",
    "    \"\"\"\n",
    "\n",
    "    def getAction(self, gameState: GameState) -> str:\n",
    "        \"\"\"Computes the Expectimax action from the current game state.\n",
    "\n",
    "        Args:\n",
    "            gameState (GameState): The current Pac-Man game state.\n",
    "\n",
    "        Returns:\n",
    "            str: The chosen action using expected utility.\n",
    "        \"\"\"\n",
    "        # TODO: Implement expectimax logic here\n",
    "        raise Exception(\"Not implemented yet\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ### Problem 4a: Better Evaluation Function (TODO)\n",
    "# ============================================================\n",
    "# This function should compute a more sophisticated evaluation function that balances\n",
    "# multiple game aspects such as food proximity, ghost distance, and capsule location.\n",
    "\n",
    "def betterEvaluationFunction(currentGameState: GameState) -> float:\n",
    "    \"\"\"Design an advanced evaluation function for Pac-Man (TODO).\n",
    "\n",
    "    Args:\n",
    "        currentGameState (GameState): The current game state.\n",
    "\n",
    "    Returns:\n",
    "        float: A more informed score estimating long-term success potential.\n",
    "\n",
    "    This function may consider:\n",
    "      - Distance to the nearest food pellet.\n",
    "      - Distance to ghosts and scared timers.\n",
    "      - Remaining capsules.\n",
    "      - Game score.\n",
    "    \"\"\"\n",
    "    # TODO: Implement improved evaluation logic\n",
    "    raise Exception(\"Not implemented yet\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ### Aliases and Instructions\n",
    "# ============================================================\n",
    "\n",
    "# Abbreviation for convenience\n",
    "better = betterEvaluationFunction\n",
    "\n",
    "# Run the game with:   python pacman.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN BLOCK - Pac-Man Agent Demonstrations\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 5: PAC-MAN MULTI-AGENT SEARCH - DEMOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n--- Pac-Man Agent Classes Implemented ---\")\n",
    "    print(\"✓ ReflexAgent: Chooses actions based on immediate successor evaluation\")\n",
    "    print(\"✓ MultiAgentSearchAgent: Base class for adversarial search agents\")\n",
    "    print(\"✓ MinimaxAgent: Implements Minimax algorithm for optimal play\")\n",
    "    print(\"✓ AlphaBetaAgent: (TODO) Alpha-Beta pruning optimization\")\n",
    "    print(\"✓ ExpectimaxAgent: (TODO) Expectimax for probabilistic ghosts\")\n",
    "    print(\"✓ betterEvaluationFunction: (TODO) Advanced state evaluation\")\n",
    "    \n",
    "    print(\"\\n--- How to Run Pac-Man Games ---\")\n",
    "    print(\"To play Pac-Man with these agents, use the following commands:\")\n",
    "    print(\"  python pacman/pacman.py                  # Manual play\")\n",
    "    print(\"  python pacman/pacman.py -p ReflexAgent   # Run Reflex Agent\")\n",
    "    print(\"  python pacman/pacman.py -p MinimaxAgent  # Run Minimax Agent\")\n",
    "    print(\"  python pacman/pacman.py -l testClassic   # Small test layout\")\n",
    "    \n",
    "    print(\"\\n--- Agent Characteristics ---\")\n",
    "    print(\"ReflexAgent:\")\n",
    "    print(\"  - Makes decisions based only on immediate successor states\")\n",
    "    print(\"  - Fast, but may not plan ahead\")\n",
    "    print(\"  - Uses evaluation function to score states\")\n",
    "    \n",
    "    print(\"\\nMinimaxAgent:\")\n",
    "    print(\"  - Assumes optimal play by all agents (Pac-Man and ghosts)\")\n",
    "    print(\"  - Plans multiple moves ahead (configurable depth)\")\n",
    "    print(\"  - Alternates between maximizing (Pac-Man) and minimizing (ghosts)\")\n",
    "    print(\"  - More strategic but slower than ReflexAgent\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ASSIGNMENT 5: IMPLEMENTATION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nNote: Full Pac-Man gameplay requires running pacman.py from terminal.\")\n",
    "    print(\"The agent classes are ready and can be tested with:\")\n",
    "    print(\"  cd pacman\")\n",
    "    print(\"  python pacman.py -p MinimaxAgent -l testClassic -k 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694c796-3af9-4a68-bd70-7401a64d29da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 6\n",
    "# Title: Course Scheduling — Group Submission\n",
    "\n",
    "**Description:**  \n",
    "This assignment presents a group implementation for the Course Scheduling problem.  \n",
    "It focuses on modeling and solving constraint satisfaction problems (CSPs) in an academic scheduling context.  \n",
    "Key tasks include:  \n",
    "- Representing courses, instructors, and time slots as variables and domains.  \n",
    "- Encoding constraints such as:  \n",
    "  - No instructor can teach two courses simultaneously  \n",
    "  - Rooms cannot host multiple courses at the same time  \n",
    "  - Students’ course conflicts are avoided  \n",
    "- Implementing a backtracking search with heuristics and forward checking.  \n",
    "\n",
    "Each section contains detailed docstrings and inline comments for clarity and readability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b22c9-6f28-4ccc-919b-9bfa4d314697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "from csp import CSP, Constraint\n",
    "\n",
    "# ### Problem 1: Chain CSP\n",
    "# In this problem, we construct a simple binary CSP arranged in a linear chain.\n",
    "# Each variable represents a node in the chain and can take values 0 or 1. Consecutive variables are connected\n",
    "# by XOR constraints, which enforce that adjacent variables must alternate in their assigned values.\n",
    "\n",
    "def make_chain_csp(n: int) -> CSP:\n",
    "    \"\"\"\n",
    "    In this function, we construct a binary chain CSP with *n* variables.\n",
    "    Each variable can take one of two possible values (0 or 1), and consecutive variables\n",
    "    are constrained using an XOR condition so that their values differ.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of variables to include in the chain.\n",
    "\n",
    "    Returns:\n",
    "        CSP: A constraint satisfaction problem instance representing the chain.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating a list of variable names (X0, X1, X2, ...)\n",
    "    variables = [f\"X{i}\" for i in range(n)]\n",
    "    \n",
    "    # Defining the domains for all variables (each can be 0 or 1)\n",
    "    domains = {var: [0, 1] for var in variables}\n",
    "\n",
    "    constraints = []\n",
    "    for i in range(n - 1):\n",
    "        # Here we define a binary constraint enforcing alternation between consecutive variables.\n",
    "        def xor_constraint(x, y):\n",
    "            return x != y\n",
    "\n",
    "        constraints.append(Constraint([variables[i], variables[i + 1]], xor_constraint))\n",
    "\n",
    "    # Returning a CSP instance constructed from variables, domains, and constraints.\n",
    "    return CSP(variables, domains, constraints)\n",
    "\n",
    "\n",
    "# ### Problem 2: N-Queens CSP\n",
    "# The goal of this section is to formulate the classic N-Queens problem as a CSP.\n",
    "# Each queen is placed in a different column, and its row position is treated as a variable value.\n",
    "# Constraints ensure that no two queens attack one another horizontally, vertically, or diagonally.\n",
    "\n",
    "def n_queens_csp(n: int) -> CSP:\n",
    "    \"\"\"\n",
    "    In this function, we create a CSP model for the N-Queens problem.\n",
    "    Each variable represents a column on the chessboard, and its domain corresponds to the possible\n",
    "    row positions available for placing a queen. Pairwise constraints between queens prevent them from\n",
    "    sharing the same row or diagonal.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the chessboard and the number of queens to place.\n",
    "\n",
    "    Returns:\n",
    "        CSP: A constraint satisfaction problem instance representing the N-Queens puzzle.\n",
    "    \"\"\"\n",
    "\n",
    "    variables = [f\"Q{i}\" for i in range(n)]  # Defining one variable per column\n",
    "    domains = {var: list(range(n)) for var in variables}  # Each variable’s domain is the set of rows\n",
    "\n",
    "    constraints = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            def queen_constraint(x, y, col_diff=j - i):\n",
    "                # Enforcing that queens cannot be in the same row or same diagonal.\n",
    "                return x != y and abs(x - y) != col_diff\n",
    "\n",
    "            constraints.append(Constraint([variables[i], variables[j]], queen_constraint))\n",
    "\n",
    "    return CSP(variables, domains, constraints)\n",
    "\n",
    "\n",
    "# ### Problem 3: Map Coloring CSP\n",
    "# In this section, we implement a CSP formulation of the Australian map-coloring problem.\n",
    "# Each region of the map is treated as a variable, and each variable can take one of three possible colors.\n",
    "# Constraints enforce that no two neighboring regions share the same color.\n",
    "\n",
    "def map_coloring_csp() -> CSP:\n",
    "    \"\"\"\n",
    "    In this function, we formulate the Australian map-coloring problem as a CSP.\n",
    "    Each region on the map is represented as a variable that can take one of three color assignments.\n",
    "    Constraints ensure that adjacent regions receive distinct colors.\n",
    "\n",
    "    Returns:\n",
    "        CSP: A constraint satisfaction problem instance representing the map coloring task.\n",
    "    \"\"\"\n",
    "\n",
    "    regions = ['WA', 'NT', 'SA', 'Q', 'NSW', 'V', 'T']  # List of Australian regions\n",
    "    colors = ['red', 'green', 'blue']  # Set of possible colors\n",
    "\n",
    "    domains = {region: colors for region in regions}\n",
    "\n",
    "    # Defining adjacency relationships based on the geographic layout of Australia.\n",
    "    neighbors = [\n",
    "        ('WA', 'NT'), ('WA', 'SA'), ('NT', 'SA'), ('NT', 'Q'),\n",
    "        ('SA', 'Q'), ('SA', 'NSW'), ('SA', 'V'), ('Q', 'NSW'), ('V', 'NSW')\n",
    "    ]\n",
    "\n",
    "    constraints = []\n",
    "    for (r1, r2) in neighbors:\n",
    "        def color_constraint(c1, c2):\n",
    "            # Adjacent regions must have different colors.\n",
    "            return c1 != c2\n",
    "\n",
    "        constraints.append(Constraint([r1, r2], color_constraint))\n",
    "\n",
    "    # Returning the CSP instance for the map coloring problem.\n",
    "    return CSP(regions, domains, constraints)\n",
    "\n",
    "\n",
    "# ### Problem 4: Course Scheduling CSP\n",
    "# In this section, we design a CSP to handle course scheduling.\n",
    "# The objective is to assign times and rooms to courses such that no instructor or room conflicts occur.\n",
    "\n",
    "def course_scheduling_csp(courses: List[str], instructors: Dict[str, str], rooms: List[str], times: List[str]) -> CSP:\n",
    "    \"\"\"\n",
    "    In this function, we construct a CSP to address the course scheduling problem.\n",
    "    Each course acts as a variable whose domain consists of all possible (room, time) combinations.\n",
    "    The constraints applied ensure that:\n",
    "      1. No two courses occupy the same room at the same time.\n",
    "      2. No instructor teaches more than one course simultaneously.\n",
    "\n",
    "    Args:\n",
    "        courses (List[str]): The list of courses to schedule.\n",
    "        instructors (Dict[str, str]): A mapping between courses and their assigned instructors.\n",
    "        rooms (List[str]): The available rooms for classes.\n",
    "        times (List[str]): The available time slots for scheduling.\n",
    "\n",
    "    Returns:\n",
    "        CSP: A constraint satisfaction problem instance representing the course scheduling scenario.\n",
    "    \"\"\"\n",
    "\n",
    "    # Each course variable’s domain consists of all (room, time) pairs.\n",
    "    variables = courses\n",
    "    domains = {course: [(room, time) for room in rooms for time in times] for course in courses}\n",
    "\n",
    "    constraints = []\n",
    "\n",
    "    # Room constraint: two courses cannot be assigned to the same room at the same time.\n",
    "    for i in range(len(courses)):\n",
    "        for j in range(i + 1, len(courses)):\n",
    "            def schedule_constraint(a, b, c1=courses[i], c2=courses[j]):\n",
    "                room1, time1 = a\n",
    "                room2, time2 = b\n",
    "                # Enforcing room exclusivity\n",
    "                if room1 == room2 and time1 == time2:\n",
    "                    return False\n",
    "                # Enforcing instructor non-overlap constraint\n",
    "                if instructors[c1] == instructors[c2] and time1 == time2:\n",
    "                    return False\n",
    "                return True\n",
    "\n",
    "            constraints.append(Constraint([courses[i], courses[j]], schedule_constraint))\n",
    "\n",
    "    # Returning the CSP instance representing the complete scheduling model.\n",
    "    return CSP(variables, domains, constraints)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN BLOCK - CSP Problem Demonstrations\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 6: CONSTRAINT SATISFACTION PROBLEMS - DEMOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Demo 1: Chain CSP\n",
    "    print(\"\\n--- Demo 1: Chain CSP ---\")\n",
    "    try:\n",
    "        n = 5\n",
    "        chain_csp = make_chain_csp(n)\n",
    "        print(f\"✓ Created Chain CSP with {n} variables\")\n",
    "        print(f\"  Variables: {chain_csp.variables}\")\n",
    "        print(f\"  Domains: each variable can be 0 or 1\")\n",
    "        print(f\"  Constraints: {len(chain_csp.constraints)} XOR constraints (adjacent vars must differ)\")\n",
    "        print(f\"  Example solution: [0, 1, 0, 1, 0] (alternating pattern)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 2: N-Queens CSP\n",
    "    print(\"\\n--- Demo 2: N-Queens CSP ---\")\n",
    "    try:\n",
    "        n_queens = 8\n",
    "        nqueens_csp = nqueens_csp_function(n_queens)\n",
    "        print(f\"✓ Created {n_queens}-Queens CSP\")\n",
    "        print(f\"  Problem: Place {n_queens} queens on {n_queens}x{n_queens} chessboard\")\n",
    "        print(f\"  Variables: {n_queens} variables (one per column)\")\n",
    "        print(f\"  Domain: rows 0 to {n_queens-1} for each queen\")\n",
    "        print(f\"  Constraints: {len(nqueens_csp.constraints)} constraints\")\n",
    "        print(f\"    - No two queens in same row\")\n",
    "        print(f\"    - No two queens on same diagonal\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 3: Map Coloring CSP\n",
    "    print(\"\\n--- Demo 3: Australian Map Coloring CSP ---\")\n",
    "    try:\n",
    "        map_csp = map_coloring_csp()\n",
    "        print(f\"✓ Created Map Coloring CSP for Australia\")\n",
    "        print(f\"  Regions: {map_csp.variables}\")\n",
    "        print(f\"  Colors available: 3 (red, green, blue)\")\n",
    "        print(f\"  Constraints: {len(map_csp.constraints)} adjacency constraints\")\n",
    "        print(f\"  Goal: Adjacent regions must have different colors\")\n",
    "        print(f\"  Example edges: WA-NT, WA-SA, NT-SA, NT-Q, SA-Q, SA-NSW, etc.\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 4: Course Scheduling CSP\n",
    "    print(\"\\n--- Demo 4: Course Scheduling CSP ---\")\n",
    "    try:\n",
    "        courses = [\"CS101\", \"CS102\", \"MATH201\"]\n",
    "        instructors = {\"CS101\": \"Dr. Smith\", \"CS102\": \"Dr. Jones\", \"MATH201\": \"Dr. Smith\"}\n",
    "        rooms = [\"Room A\", \"Room B\"]\n",
    "        times = [\"9:00 AM\", \"11:00 AM\", \"2:00 PM\"]\n",
    "        \n",
    "        schedule_csp = course_scheduling_csp(courses, instructors, rooms, times)\n",
    "        print(f\"✓ Created Course Scheduling CSP\")\n",
    "        print(f\"  Courses: {courses}\")\n",
    "        print(f\"  Instructors: {list(set(instructors.values()))}\")\n",
    "        print(f\"  Rooms: {rooms}\")\n",
    "        print(f\"  Times: {times}\")\n",
    "        print(f\"  Domain size per course: {len(rooms) * len(times)} (room, time) pairs\")\n",
    "        print(f\"  Constraints:\")\n",
    "        print(f\"    - No two courses in same room at same time\")\n",
    "        print(f\"    - Instructors can't teach multiple courses simultaneously\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n--- CSP Problem Summary ---\")\n",
    "    print(\"All four CSP problem types successfully defined:\")\n",
    "    print(\"  1. Chain CSP: Binary constraints with alternating values\")\n",
    "    print(\"  2. N-Queens: Classic combinatorial problem\")\n",
    "    print(\"  3. Map Coloring: Graph coloring on Australian map\")\n",
    "    print(\"  4. Course Scheduling: Real-world scheduling with multiple constraints\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ASSIGNMENT 6: ALL DEMOS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nNote: To solve these CSPs, use a backtracking search algorithm with:\")\n",
    "    print(\"  - Variable ordering heuristics (e.g., MRV)\")\n",
    "    print(\"  - Value ordering heuristics (e.g., least constraining value)\")\n",
    "    print(\"  - Inference (e.g., forward checking, AC-3)\")\n",
    "    print(\"\\nRun grader with: python scheduling/grader.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10df268-a9b9-490f-bd1d-d4c35367cf82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 7\n",
    "# Title: Car Tracking — Group Submission\n",
    "\n",
    "**Description:**  \n",
    "This assignment implements a probabilistic car tracking system in a grid environment.  \n",
    "The main objectives are:  \n",
    "- Maintain and update a belief distribution for a car’s position.  \n",
    "- Incorporate noisy sensor measurements (sonar readings) into the belief.  \n",
    "- Use a motion model to predict the car’s movement.  \n",
    "- Apply exact inference and Gaussian updates for accurate position estimation.  \n",
    "\n",
    "Each function and class includes detailed docstrings and inline comments for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b3591-893d-4b0a-a57c-b4dcfb096aaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutil\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconst\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Const\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Belief\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the car directory to the path so we can import its modules\n",
    "if 'car' not in sys.path:\n",
    "    car_path = os.path.join(os.getcwd(), 'car')\n",
    "    if os.path.exists(car_path):\n",
    "        sys.path.insert(0, car_path)\n",
    "\n",
    "import util\n",
    "from engine.const import Const\n",
    "from util import Belief\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ### Class: ExactInference\n",
    "# In this class, we implement the core logic for maintaining and updating the belief\n",
    "# distribution over a grid of possible car locations. The approach uses **exact Bayesian updates**,\n",
    "# meaning all probabilities are computed precisely (not approximated), which ensures correctness\n",
    "# but can be computationally expensive for large maps.\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "class ExactInference:\n",
    "    \"\"\" \n",
    "    In this class, we implement an exact inference algorithm that maintains a belief distribution\n",
    "    over the probability of a car being located in each grid cell. The algorithm performs updates\n",
    "    based on sensor observations (sonar readings) and the car’s transition model over time.\n",
    "\n",
    "    This model is accurate but computationally intensive because it explicitly enumerates all\n",
    "    possible transitions and outcomes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numRows: int, numCols: int):\n",
    "        \"\"\"\n",
    "        Constructor that initializes an ExactInference object with a grid of size numRows × numCols.\n",
    "        We initialize a uniform belief distribution across all possible tiles.\n",
    "\n",
    "        Args:\n",
    "            numRows (int): The number of rows in the map grid.\n",
    "            numCols (int): The number of columns in the map grid.\n",
    "        \"\"\"\n",
    "        self.skipElapse = False  # Used internally for testing when time-elapse updates are disabled.\n",
    "        self.belief = util.Belief(numRows, numCols)  # The current belief state (probability distribution)\n",
    "        self.transProb = util.loadTransProb()  # The learned transition probabilities between tiles\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ### Problem 1: Observation Update\n",
    "    # In this method, we update our belief distribution based on a noisy distance observation.\n",
    "    # The observed distance is modeled as the true distance corrupted by Gaussian noise.\n",
    "    # We update each tile’s probability proportional to how likely the observed distance would\n",
    "    # have been if the car were actually located there.\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    def observe(self, agentX: int, agentY: int, observedDist: float) -> None:\n",
    "        \"\"\"\n",
    "        This function updates the belief distribution based on a new sonar observation.\n",
    "        We assume the measurement follows a Gaussian distribution centered at the true distance\n",
    "        with standard deviation Const.SONAR_STD.\n",
    "\n",
    "        Args:\n",
    "            agentX (int): The agent’s x-coordinate (the observer’s car position).\n",
    "            agentY (int): The agent’s y-coordinate (the observer’s car position).\n",
    "            observedDist (float): The noisy observed distance to the tracked car.\n",
    "        \"\"\"\n",
    "        for row in range(self.belief.numRows):\n",
    "            for col in range(self.belief.numCols):\n",
    "                # Convert the grid coordinates into spatial coordinates.\n",
    "                carX = util.colToX(col)\n",
    "                carY = util.rowToY(row)\n",
    "\n",
    "                # Compute the expected distance from the agent to this cell.\n",
    "                expectedDist = math.sqrt((agentX - carX) ** 2 + (agentY - carY) ** 2)\n",
    "\n",
    "                # Weight the prior probability by how likely this distance is under the Gaussian noise model.\n",
    "                self.belief.setProb(row, col, self.belief.getProb(row, col) * util.pdf(expectedDist, Const.SONAR_STD, observedDist))\n",
    "\n",
    "        # Normalize to ensure the probabilities sum to 1.\n",
    "        self.belief.normalize()\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ### Problem 2: Time Elapse Update\n",
    "    # This method predicts the car’s new belief distribution after one time step,\n",
    "    # based on the learned transition probabilities between grid cells.\n",
    "    # We apply the transition model to propagate probabilities forward in time.\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    def elapseTime(self) -> None:\n",
    "        \"\"\"\n",
    "        This function updates the belief distribution after a time step has elapsed.\n",
    "        It uses the learned transition probabilities to compute the posterior belief\n",
    "        over new possible car positions.\n",
    "        \"\"\"\n",
    "        if self.skipElapse:\n",
    "            return\n",
    "\n",
    "        # Create a new belief distribution initialized to zero.\n",
    "        newBelief = util.Belief(self.belief.numRows, self.belief.numCols, 0.0)\n",
    "\n",
    "        # For every possible transition, move probability mass according to transition likelihood.\n",
    "        for (oldTile, newTile), prob in self.transProb.items():\n",
    "            oldRow, oldCol = oldTile\n",
    "            newRow, newCol = newTile\n",
    "\n",
    "            newBelief.addProb(newRow, newCol, self.belief.getProb(oldRow, oldCol) * prob)\n",
    "\n",
    "        # Replace the old belief with the updated one and normalize.\n",
    "        self.belief = newBelief\n",
    "        self.belief.normalize()\n",
    "\n",
    "\n",
    "    def getBelief(self) -> Belief:\n",
    "        \"\"\"\n",
    "        Returns the current belief distribution over the grid.\n",
    "        The returned belief object contains probabilities that sum to 1.\n",
    "\n",
    "        Returns:\n",
    "            Belief: The current normalized belief distribution.\n",
    "        \"\"\"\n",
    "        return self.belief\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ### Class: ExactInferenceWithSensorDeception\n",
    "# This subclass extends ExactInference by introducing *sensor deception*, a perturbation applied\n",
    "# to the observed distances to simulate adversarial or faulty sensor readings. This adjustment\n",
    "# skews the observed data before it is used in the standard observation update procedure.\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "class ExactInferenceWithSensorDeception(ExactInference):\n",
    "    \"\"\"\n",
    "    In this class, we extend the ExactInference model by introducing sensor deception.\n",
    "    A skewness factor modifies the observed distance before updating the belief,\n",
    "    simulating a sensor that produces biased or tampered readings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numRows: int, numCols: int, skewness: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initializes the sensor deception inference model with a given skewness parameter.\n",
    "\n",
    "        Args:\n",
    "            numRows (int): The number of rows in the grid.\n",
    "            numCols (int): The number of columns in the grid.\n",
    "            skewness (float): Factor controlling the distortion applied to observed distances.\n",
    "        \"\"\"\n",
    "        super().__init__(numRows, numCols)\n",
    "        self.skewness = skewness\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # ### Problem 4: Observation with Sensor Deception\n",
    "    # Here, we modify the observed distance by applying a transformation based on the skewness factor.\n",
    "    # The adjusted observation is then processed using the standard observation update from the parent class.\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    def observe(self, agentX: int, agentY: int, observedDist: float) -> None:\n",
    "        \"\"\"\n",
    "        Updates the belief distribution based on a deceptive (skewed) observation.\n",
    "\n",
    "        The observed distance is transformed using the given skewness factor before applying\n",
    "        the regular observation update. This simulates a scenario in which the sensor provides\n",
    "        manipulated distance measurements.\n",
    "\n",
    "        Args:\n",
    "            agentX (int): The agent’s x-coordinate.\n",
    "            agentY (int): The agent’s y-coordinate.\n",
    "            observedDist (float): The raw (potentially deceptive) observed distance.\n",
    "        \"\"\"\n",
    "        # Apply the skew transformation to simulate sensor deception.\n",
    "        skewFactor = 1.0 / (1.0 + self.skewness**2)\n",
    "        transformedDist = skewFactor * observedDist + math.sqrt(2 * skewFactor)\n",
    "\n",
    "        # Use the standard observation update from the parent class with the transformed distance.\n",
    "        super().observe(agentX, agentY, transformedDist)\n",
    "\n",
    "\n",
    "    def elapseTime(self) -> None:\n",
    "        \"\"\"Applies the standard time elapse update from the parent class.\"\"\"\n",
    "        super().elapseTime()\n",
    "\n",
    "    def getBelief(self) -> Belief:\n",
    "        \"\"\"Returns the current belief distribution, identical to the parent class implementation.\"\"\"\n",
    "        return super().getBelief()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# ### Usage Instructions\n",
    "# The following are command-line examples for running different car tracking scenarios.\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "    For stationary car tracking:\n",
    "        python drive.py -a -p -d -k 1 -i exactInference\n",
    "\n",
    "    For moving car tracking:\n",
    "        python drive.py -a -d -k 1 -i exactInference\n",
    "\n",
    "    For multiple cars on Lombard:\n",
    "        python drive.py -a -d -k 3 -i exactInference -l lombard\n",
    "\n",
    "    For sensor deception scenario:\n",
    "        python drive.py -a -p -d -k 3 -i exactInferenceWithSensorDeception\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c978c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN BLOCK - Car Tracking Demonstrations\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Run in notebook\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ASSIGNMENT 7: CAR TRACKING WITH EXACT INFERENCE - DEMOS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n--- Car Tracking Classes Implemented ---\")\n",
    "    print(\"✓ ExactInference: Bayesian belief tracking with sensor observations\")\n",
    "    print(\"✓ ExactInferenceWithSensorDeception: Handles biased/faulty sensors\")\n",
    "    \n",
    "    # Demo 1: ExactInference Instantiation\n",
    "    print(\"\\n--- Demo 1: Creating Exact Inference Tracker ---\")\n",
    "    try:\n",
    "        numRows, numCols = 10, 10\n",
    "        tracker = ExactInference(numRows, numCols)\n",
    "        print(f\"✓ Created ExactInference tracker\")\n",
    "        print(f\"  Grid size: {numRows} x {numCols}\")\n",
    "        print(f\"  Initial belief: Uniform distribution\")\n",
    "        print(f\"  Transition model: Loaded from learned probabilities\")\n",
    "        \n",
    "        # Check initial belief\n",
    "        initial_belief = tracker.getBelief()\n",
    "        print(f\"  Total probability mass: {sum(initial_belief.getProb(r, c) for r in range(numRows) for c in range(numCols)):.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 2: Observation Update\n",
    "    print(\"\\n--- Demo 2: Belief Update After Observation ---\")\n",
    "    try:\n",
    "        # Simulate an observation\n",
    "        agentX, agentY = 50, 50  # Agent position\n",
    "        observedDist = 30.0  # Observed distance to car\n",
    "        \n",
    "        print(f\"Agent position: ({agentX}, {agentY})\")\n",
    "        print(f\"Observed distance: {observedDist}\")\n",
    "        print(f\"Updating belief based on observation...\")\n",
    "        \n",
    "        tracker.observe(agentX, agentY, observedDist)\n",
    "        print(f\"✓ Observation incorporated into belief\")\n",
    "        \n",
    "        # Find highest probability cell\n",
    "        belief = tracker.getBelief()\n",
    "        max_prob = 0\n",
    "        max_cell = (0, 0)\n",
    "        for r in range(numRows):\n",
    "            for c in range(numCols):\n",
    "                prob = belief.getProb(r, c)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_cell = (r, c)\n",
    "        \n",
    "        print(f\"  Most likely cell: {max_cell} with probability {max_prob:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 3: Time Elapse Update\n",
    "    print(\"\\n--- Demo 3: Belief Propagation Over Time ---\")\n",
    "    try:\n",
    "        print(\"Simulating time elapse (car movement)...\")\n",
    "        tracker.elapseTime()\n",
    "        print(f\"✓ Time elapse update applied\")\n",
    "        print(f\"  Belief propagated using transition model\")\n",
    "        \n",
    "        belief_after = tracker.getBelief()\n",
    "        total_prob = sum(belief_after.getProb(r, c) for r in range(numRows) for c in range(numCols))\n",
    "        print(f\"  Total probability mass after elapse: {total_prob:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 3: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Demo 4: Sensor Deception Tracker\n",
    "    print(\"\\n--- Demo 4: Exact Inference with Sensor Deception ---\")\n",
    "    try:\n",
    "        skewness = 0.5\n",
    "        deceptive_tracker = ExactInferenceWithSensorDeception(numRows, numCols, skewness=skewness)\n",
    "        print(f\"✓ Created deceptive sensor tracker\")\n",
    "        print(f\"  Skewness factor: {skewness}\")\n",
    "        print(f\"  This simulates biased/faulty sensor readings\")\n",
    "        \n",
    "        # Test deceptive observation\n",
    "        print(f\"\\nApplying deceptive observation...\")\n",
    "        deceptive_tracker.observe(agentX, agentY, observedDist)\n",
    "        print(f\"✓ Deceptive observation processed\")\n",
    "        print(f\"  Distance was transformed before belief update\")\n",
    "        print(f\"  Formula: dist' = (1/(1+s²)) * dist + sqrt(2/(1+s²))\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in Demo 4: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n--- How Car Tracking Works ---\")\n",
    "    print(\"1. Observation Update (Bayes Rule):\")\n",
    "    print(\"   - Measure noisy distance from agent to car\")\n",
    "    print(\"   - Update belief based on Gaussian sensor model\")\n",
    "    print(\"   - P(car at cell | observation) ∝ P(observation | car at cell) * P(car at cell)\")\n",
    "    \n",
    "    print(\"\\n2. Time Elapse Update (Prediction):\")\n",
    "    print(\"   - Use learned transition probabilities\")\n",
    "    print(\"   - Predict where car might move next\")\n",
    "    print(\"   - P(car at cell_t+1) = Σ P(cell_t+1 | cell_t) * P(car at cell_t)\")\n",
    "    \n",
    "    print(\"\\n3. Sensor Deception:\")\n",
    "    print(\"   - Models adversarial or faulty sensors\")\n",
    "    print(\"   - Applies transformation to observed distances\")\n",
    "    print(\"   - Makes tracking more challenging\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ASSIGNMENT 7: ALL DEMOS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nNote: Full car tracking simulation requires running drive.py:\")\n",
    "    print(\"  cd car\")\n",
    "    print(\"  python drive.py -a -d -k 1 -i exactInference\")\n",
    "    print(\"  python drive.py -a -p -d -k 3 -i exactInferenceWithSensorDeception\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b927b5b-2a4b-4901-a567-ed610ecc8fce",
   "metadata": {},
   "source": [
    "# MAIN TO RUN ALL FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1a95bc-4cc4-4a2f-9a71-4d98bd78973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robotics\\miniconda3\\envs\\cs221\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robotics\\miniconda3\\envs\\cs221\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Import assignment modules\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'foundations'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'sentiment'))\n",
    "import foundations.submission as foundations\n",
    "import sentiment.submission as sentiment\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "BLUE = (30, 144, 255)\n",
    "GREEN = (50, 205, 50)\n",
    "RED = (255, 69, 0)\n",
    "\n",
    "font = pygame.font.Font(None, 48)\n",
    "small_font = pygame.font.Font(None, 28)\n",
    "\n",
    "# Screen setup\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Stanford CS221 Assignments Tester\")\n",
    "\n",
    "# Helper Functions\n",
    "def draw_text(surface, text, font, color, x, y):\n",
    "    text_obj = font.render(text, True, color)\n",
    "    surface.blit(text_obj, (x, y))\n",
    "\n",
    "def draw_button(surface, text, x, y, w, h, inactive_color, active_color, mouse_pos):\n",
    "    if x < mouse_pos[0] < x + w and y < mouse_pos[1] < y + h:\n",
    "        pygame.draw.rect(surface, active_color, (x, y, w, h))\n",
    "    else:\n",
    "        pygame.draw.rect(surface, inactive_color, (x, y, w, h))\n",
    "    draw_text(surface, text, small_font, WHITE, x + 10, y + 10)\n",
    "\n",
    "# -----------------------------\n",
    "# Main Menu\n",
    "# -----------------------------\n",
    "def main_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        \"1. Foundations\",\n",
    "        \"2. Sentiment Analysis\",\n",
    "        \"3. Pacman\",\n",
    "        \"4. Mountaincar\",\n",
    "        \"5. Scheduling\",\n",
    "        \"6. Route Finding\",\n",
    "        \"7. Car Tracking\",\n",
    "        \"8. Quit\"\n",
    "    ]\n",
    "    \n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"CS221 Assignments\", font, WHITE, 200, 50)\n",
    "        \n",
    "        for i, option in enumerate(options):\n",
    "            draw_button(screen, option, 250, 150 + i * 60, 300, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if 250 < mouse_pos[0] < 550:\n",
    "                    if 150 < mouse_pos[1] < 200:\n",
    "                        foundations_menu(screen)\n",
    "                    elif 210 < mouse_pos[1] < 260:\n",
    "                        sentiment_menu(screen)\n",
    "                    elif 270 < mouse_pos[1] < 320:\n",
    "                        pacman_menu(screen)\n",
    "                    elif 330 < mouse_pos[1] < 380:\n",
    "                        mountaincar_menu(screen)\n",
    "                    elif 390 < mouse_pos[1] < 440:\n",
    "                        scheduling_menu(screen)\n",
    "                    elif 450 < mouse_pos[1] < 500:\n",
    "                        route_menu(screen)\n",
    "                    elif 510 < mouse_pos[1] < 560:\n",
    "                        car_menu(screen)\n",
    "                    elif 570 < mouse_pos[1] < 620:\n",
    "                        running = False\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Foundations Menu - Interactive GUI with text input\n",
    "# -----------------------------\n",
    "def foundations_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        \"1. Find First Word\",\n",
    "        \"2. Euclidean Distance\",\n",
    "        \"3. Mutate Sentences\",\n",
    "        \"4. Dot Product\",\n",
    "        \"5. Increment Vector\",\n",
    "        \"6. Find Duplicate Words\",\n",
    "        \"7. Run Full Grader Tests\",\n",
    "        \"8. Back\"\n",
    "    ]\n",
    "    \n",
    "    selected_test = None\n",
    "    input_text = \"\"\n",
    "    input_text2 = \"\"\n",
    "    result_lines = []\n",
    "    input_active = False\n",
    "    input_box = pygame.Rect(100, 240, 600, 40)\n",
    "    input_box2 = pygame.Rect(100, 320, 600, 40)\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        \n",
    "        if selected_test is None:\n",
    "            # Main menu\n",
    "            draw_text(screen, \"Foundations Tests\", font, WHITE, 220, 30)\n",
    "            for i, option in enumerate(options):\n",
    "                draw_button(screen, option, 250, 100 + i * 60, 300, 50, BLUE, GREEN, mouse_pos)\n",
    "        else:\n",
    "            # Input screen\n",
    "            draw_text(screen, selected_test, small_font, WHITE, 50, 30)\n",
    "            \n",
    "            # Instructions based on test type\n",
    "            if \"First Word\" in selected_test:\n",
    "                draw_text(screen, \"Enter text (words separated by spaces):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            elif \"Euclidean\" in selected_test:\n",
    "                draw_text(screen, \"Point 1 (x,y): e.g., 0,0\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"Point 2 (x,y): e.g., 3,4\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            elif \"Mutate\" in selected_test:\n",
    "                draw_text(screen, \"Enter sentence (e.g., 'a b c'):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            elif \"Dot Product\" in selected_test:\n",
    "                draw_text(screen, \"Vector 1: e.g., a:1,b:2\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"Vector 2: e.g., b:3,c:4\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            elif \"Increment\" in selected_test:\n",
    "                draw_text(screen, \"V1 and Scale: e.g., a:1,b:2 2\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"V2: e.g., b:3,c:4\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            elif \"Duplicate\" in selected_test:\n",
    "                draw_text(screen, \"Enter text:\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            \n",
    "            # Compute and Back buttons\n",
    "            draw_button(screen, \"Compute\", 200, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            draw_button(screen, \"Back\", 450, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            \n",
    "            # Display results\n",
    "            y_offset = 450\n",
    "            for i, line in enumerate(result_lines[-6:]):\n",
    "                draw_text(screen, line[:80], small_font, GREEN, 50, y_offset + i * 25)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            \n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if selected_test is None:\n",
    "                    # Menu selection\n",
    "                    for i in range(8):\n",
    "                        if 250 < mouse_pos[0] < 550 and 100 + i * 60 < mouse_pos[1] < 150 + i * 60:\n",
    "                            if i == 6:  # Run grader\n",
    "                                result_lines = [\"Opening terminal to run grader tests...\"]\n",
    "                                pygame.display.flip()\n",
    "                                foundations_path = os.path.join(os.getcwd(), \"foundations\")\n",
    "                                cmd = f'cd \"{foundations_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            elif i == 7:\n",
    "                                running = False\n",
    "                            else:\n",
    "                                selected_test = options[i]\n",
    "                                input_text = \"\"\n",
    "                                input_text2 = \"\"\n",
    "                                result_lines = []\n",
    "                            break\n",
    "                else:\n",
    "                    # Check if clicking in input boxes\n",
    "                    input_active = input_box.collidepoint(event.pos) or input_box2.collidepoint(event.pos)\n",
    "                    \n",
    "                    # Compute button\n",
    "                    if 200 < mouse_pos[0] < 350 and 380 < mouse_pos[1] < 430:\n",
    "                        try:\n",
    "                            from collections import defaultdict\n",
    "                            result_lines = []\n",
    "                            \n",
    "                            if \"First Word\" in selected_test:\n",
    "                                result = foundations.find_alphabetically_first_word(input_text)\n",
    "                                result_lines.append(f\"Input: {input_text}\")\n",
    "                                result_lines.append(f\"First word alphabetically: {result}\")\n",
    "                            elif \"Euclidean\" in selected_test:\n",
    "                                p1 = tuple(map(float, input_text.split(',')))\n",
    "                                p2 = tuple(map(float, input_text2.split(',')))\n",
    "                                result = foundations.euclidean_distance(p1, p2)\n",
    "                                result_lines.append(f\"Point 1: {p1}, Point 2: {p2}\")\n",
    "                            \n",
    "                            elif \"Mutate\" in selected_test:\n",
    "                                result = foundations.mutate_sentences(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}'\")\n",
    "                                result_lines.append(f\"Found {len(result)} mutations:\")\n",
    "                                for r in result[:5]:\n",
    "                                    result_lines.append(f\"  {r}\")\n",
    "                                if len(result) > 5:\n",
    "                                    result_lines.append(f\"  ... and {len(result)-5} more\")\n",
    "                            \n",
    "                            elif \"Dot Product\" in selected_test:\n",
    "                                v1 = defaultdict(float)\n",
    "                                for pair in input_text.split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v1[k.strip()] = float(v.strip())\n",
    "                                v2 = defaultdict(float)\n",
    "                                for pair in input_text2.split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v2[k.strip()] = float(v.strip())\n",
    "                                result = foundations.sparse_vector_dot_product(v1, v2)\n",
    "                                result_lines.append(f\"V1: {dict(v1)}\")\n",
    "                                result_lines.append(f\"V2: {dict(v2)}\")\n",
    "                                result_lines.append(f\"Dot Product: {result}\")\n",
    "                            \n",
    "                            elif \"Increment\" in selected_test:\n",
    "                                parts = input_text.split()\n",
    "                                scale = float(parts[-1])\n",
    "                                v1 = defaultdict(float)\n",
    "                                for pair in ' '.join(parts[:-1]).split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v1[k.strip()] = float(v.strip())\n",
    "                                v2 = defaultdict(float)\n",
    "                                for pair in input_text2.split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v2[k.strip()] = float(v.strip())\n",
    "                                result_lines.append(f\"Before: {dict(v1)}\")\n",
    "                                foundations.increment_sparse_vector(v1, scale, v2)\n",
    "                                result_lines.append(f\"After += {scale}*V2: {dict(v1)}\")\n",
    "                            \n",
    "                            elif \"Duplicate\" in selected_test:\n",
    "                                result = foundations.find_nonsingleton_words(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}'\")\n",
    "                                result_lines.append(f\"Words appearing >1: {result}\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            result_lines = [f\"Error: {str(e)}\", \"Check your input format\"]\n",
    "                    \n",
    "                    # Back button\n",
    "                    if 450 < mouse_pos[0] < 600 and 380 < mouse_pos[1] < 430:\n",
    "                        selected_test = None\n",
    "                        input_text = \"\"\n",
    "                        input_text2 = \"\"\n",
    "                        result_lines = []\n",
    "            \n",
    "            if event.type == pygame.KEYDOWN and selected_test is not None:\n",
    "                if input_box.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text = input_text[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text += event.unicode\n",
    "                elif input_box2.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text2 = input_text2[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text2 += event.unicode\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Sentiment Analysis Menu - Interactive GUI with text input\n",
    "# -----------------------------\n",
    "def sentiment_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        \"1. Extract Word Features\",\n",
    "        \"2. Character N-Grams (n=3)\",\n",
    "        \"3. Test K-Means Clustering\",\n",
    "        \"4. Run Full Grader Tests\",\n",
    "        \"5. Back\"\n",
    "    ]\n",
    "    \n",
    "    selected_test = None\n",
    "    input_text = \"\"\n",
    "    input_text2 = \"\"\n",
    "    result_lines = []\n",
    "    input_box = pygame.Rect(100, 240, 600, 40)\n",
    "    input_box2 = pygame.Rect(100, 320, 600, 40)\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        \n",
    "        if selected_test is None:\n",
    "            # Main menu\n",
    "            draw_text(screen, \"Sentiment Analysis\", font, WHITE, 200, 30)\n",
    "            for i, option in enumerate(options):\n",
    "                draw_button(screen, option, 250, 100 + i * 60, 300, 50, BLUE, GREEN, mouse_pos)\n",
    "        else:\n",
    "            # Input screen\n",
    "            draw_text(screen, selected_test, small_font, WHITE, 50, 30)\n",
    "            \n",
    "            # Instructions based on test type\n",
    "            if \"Word Features\" in selected_test:\n",
    "                draw_text(screen, \"Enter text (e.g., 'I am what I am'):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            elif \"N-Grams\" in selected_test:\n",
    "                draw_text(screen, \"Enter text (e.g., 'I like tacos'):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"N value (default=3):\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70] if input_text2 else \"3\", small_font, WHITE, 105, 325)\n",
    "            elif \"K-Means\" in selected_test:\n",
    "                draw_text(screen, \"K (number of clusters, e.g., 2):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"Max epochs (e.g., 10):\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            \n",
    "            # Compute and Back buttons\n",
    "            draw_button(screen, \"Compute\", 200, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            draw_button(screen, \"Back\", 450, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            \n",
    "            # Display results\n",
    "            y_offset = 450\n",
    "            for i, line in enumerate(result_lines[-6:]):\n",
    "                draw_text(screen, line[:80], small_font, GREEN, 50, y_offset + i * 25)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            \n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if selected_test is None:\n",
    "                    # Menu selection\n",
    "                    for i in range(5):\n",
    "                        if 250 < mouse_pos[0] < 550 and 100 + i * 60 < mouse_pos[1] < 150 + i * 60:\n",
    "                            if i == 3:  # Run grader\n",
    "                                result_lines = [\"Opening terminal to run grader tests...\"]\n",
    "                                pygame.display.flip()\n",
    "                                sentiment_path = os.path.join(os.getcwd(), \"sentiment\")\n",
    "                                cmd = f'cd \"{sentiment_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            elif i == 4:\n",
    "                                running = False\n",
    "                            else:\n",
    "                                selected_test = options[i]\n",
    "                                input_text = \"\"\n",
    "                                input_text2 = \"\"\n",
    "                                result_lines = []\n",
    "                            break\n",
    "                else:\n",
    "                    # Compute button\n",
    "                    if 200 < mouse_pos[0] < 350 and 380 < mouse_pos[1] < 430:\n",
    "                        try:\n",
    "                            result_lines = []\n",
    "                            \n",
    "                            if \"Word Features\" in selected_test:\n",
    "                                result = sentiment.extractWordFeatures(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}'\")\n",
    "                                result_lines.append(f\"Features: {result}\")\n",
    "                                result_lines.append(f\"Total unique words: {len(result)}\")\n",
    "                            \n",
    "                            elif \"N-Grams\" in selected_test:\n",
    "                                n = int(input_text2) if input_text2 else 3\n",
    "                                extractor = sentiment.extractCharacterFeatures(n)\n",
    "                                result = extractor(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}' (n={n})\")\n",
    "                                result_lines.append(f\"Char n-grams: {list(result.items())[:10]}\")\n",
    "                                if len(result) > 10:\n",
    "                                    result_lines.append(f\"... and {len(result)-10} more\")\n",
    "                                result_lines.append(f\"Total {n}-grams: {len(result)}\")\n",
    "                            \n",
    "                            elif \"K-Means\" in selected_test:\n",
    "                                K = int(input_text) if input_text else 2\n",
    "                                maxEpochs = int(input_text2) if input_text2 else 10\n",
    "                                # Generate sample data\n",
    "                                examples = [{'x': i, 'y': i%2} for i in range(10)]\n",
    "                                centers, assignments, loss = sentiment.kmeans(examples, K, maxEpochs)\n",
    "                                result_lines.append(f\"K={K}, Max Epochs={maxEpochs}\")\n",
    "                                result_lines.append(f\"Final loss: {loss:.4f}\")\n",
    "                                result_lines.append(f\"Assignments: {assignments}\")\n",
    "                                result_lines.append(f\"Cluster centers: {centers[:2]}...\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            result_lines = [f\"Error: {str(e)}\", \"Check your input format\"]\n",
    "                    \n",
    "                    # Back button\n",
    "                    if 450 < mouse_pos[0] < 600 and 380 < mouse_pos[1] < 430:\n",
    "                        selected_test = None\n",
    "                        input_text = \"\"\n",
    "                        input_text2 = \"\"\n",
    "                        result_lines = []\n",
    "            \n",
    "            if event.type == pygame.KEYDOWN and selected_test is not None:\n",
    "                if input_box.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text = input_text[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text += event.unicode\n",
    "                elif input_box2.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text2 = input_text2[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text2 += event.unicode\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Pacman Menu - Opens game in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def pacman_menu(screen):\n",
    "    running = True\n",
    "    agents = [\n",
    "        (\"Reflex Agent\", \"ReflexAgent\", None),\n",
    "        (\"Minimax Agent\", \"MinimaxAgent\", None),\n",
    "        (\"Alpha-Beta Agent\", \"AlphaBetaAgent\", None),\n",
    "        (\"Expectimax Agent\", \"ExpectimaxAgent\", None),\n",
    "        (\"Run Full Grader Tests\", \"grader\", None),\n",
    "        (\"Back to Main Menu\", None, None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Pacman Agents\", font, WHITE, 250, 50)\n",
    "\n",
    "        for i, (name, _, _) in enumerate(agents):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 200, 120 + i * 60, 400, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 100, 450)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, agent_type, _) in enumerate(agents):\n",
    "                    if 200 < mouse_pos[0] < 600 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        try:\n",
    "                            if agent_type is None:\n",
    "                                running = False\n",
    "                                break\n",
    "                            if agent_type == \"grader\":\n",
    "                                result = \"Opening terminal to run grader tests...\"\n",
    "                                pygame.display.flip()\n",
    "                                pacman_path = os.path.join(os.getcwd(), \"pacman\")\n",
    "                                cmd = f'cd \"{pacman_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                result = \"Grader tests running! Check terminal window\"\n",
    "                                break\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            # Open in new PowerShell window\n",
    "                            pacman_path = os.path.join(os.getcwd(), \"pacman\")\n",
    "                            cmd = f'cd \"{pacman_path}\" ; python pacman.py -p {agent_type} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} is playing! Watch the game window\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Mountaincar Menu - Opens training in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def mountaincar_menu(screen):\n",
    "    running = True\n",
    "    agents = [\n",
    "        (\"Value Iteration\", \"--agent\", \"value-iteration\"),\n",
    "        (\"Tabular Q-Learning\", \"--agent\", \"tabular\"),\n",
    "        (\"Function Approximation\", \"--agent\", \"function-approximation\"),\n",
    "        (\"Constrained Q-Learning\", \"--agent\", \"constrained\"),\n",
    "        (\"Run Full Grader Tests\", \"grader\", None),\n",
    "        (\"Back to Main Menu\", None, None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Mountaincar Agents\", font, WHITE, 250, 50)\n",
    "\n",
    "        for i, (name, _, _) in enumerate(agents):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 200, 120 + i * 60, 400, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 100, 450)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, flag_name, flag_value) in enumerate(agents):\n",
    "                    if 200 < mouse_pos[0] < 600 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        if flag_name is None:  # Back to main menu\n",
    "                            running = False\n",
    "                            break\n",
    "                        if flag_name == \"visualize\":  # Run visualization\n",
    "                            result = f\"Starting {name} visualization...\"\n",
    "                            pygame.display.flip()\n",
    "                            mountaincar_path = os.path.join(os.getcwd(), \"mountaincar\")\n",
    "                            cmd = f'cd \"{mountaincar_path}\" ; python mountaincar.py --agent {flag_value} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Watch the visualization window\"\n",
    "                            break\n",
    "                        if flag_name == \"grader\":  # Run grader tests\n",
    "                            result = \"Opening terminal to run grader tests...\"\n",
    "                            pygame.display.flip()\n",
    "                            mountaincar_path = os.path.join(os.getcwd(), \"mountaincar\")\n",
    "                            cmd = f'cd \"{mountaincar_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = \"Grader tests running! Check terminal window\"\n",
    "                            break\n",
    "                        try:\n",
    "                            result = f\"Starting {name}... Check new terminal window!\"\n",
    "                            pygame.display.flip()\n",
    "                            # Open in new PowerShell window so training output and plots are visible\n",
    "                            mountaincar_path = os.path.join(os.getcwd(), \"mountaincar\")\n",
    "                            cmd = f'cd \"{mountaincar_path}\" ; python train.py {flag_name} {flag_value} ; Start-Sleep -Seconds 1 ; Get-ChildItem -Filter \"*.png\" | Sort-Object LastWriteTime -Descending | Select-Object -First 3 | ForEach-Object {{ Start-Process $_.FullName }} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd], \n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} is training! Plots will open automatically\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Scheduling Menu - Opens scheduling tests in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def scheduling_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        (\"Run N-Queens Test\", \"test_nqueens.py\"),\n",
    "        (\"Run Problem 2 Tests\", \"run_p2.py\"),\n",
    "        (\"Run Problem 3 Tests\", \"run_p3.py\"),\n",
    "        (\"Run Full Grader Tests\", \"grader.py\"),\n",
    "        (\"Back to Main Menu\", None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Scheduling & CSP\", font, WHITE, 220, 50)\n",
    "\n",
    "        for i, (name, _) in enumerate(options):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 200, 120 + i * 60, 400, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 100, 450)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, script) in enumerate(options):\n",
    "                    if 200 < mouse_pos[0] < 600 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        if script is None:\n",
    "                            running = False\n",
    "                            break\n",
    "                        try:\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            scheduling_path = os.path.join(os.getcwd(), \"scheduling\")\n",
    "                            cmd = f'cd \"{scheduling_path}\" ; python {script} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Watch terminal window\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Route Menu - RUNS route demos and visualization in NEW TERMINAL WINDOW with conda run\n",
    "# -----------------------------\n",
    "def route_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        (\"Shortest (UCS)\", \"shortest\", \"ucs\"),\n",
    "        (\"Shortest (A*)\", \"shortest\", \"astar\"),\n",
    "        (\"Waypoints (UCS)\", \"waypoints\", \"ucs\"),\n",
    "        (\"Visualize Last Path\", None, None),\n",
    "        (\"Run Full Grader Tests\", \"grader\", None),\n",
    "        (\"Back to Main Menu\", None, None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Route Planning\", font, WHITE, 250, 50)\n",
    "\n",
    "        for i, (name, _, _) in enumerate(options):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 150, 120 + i * 60, 500, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 80, 500)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, demo, method) in enumerate(options):\n",
    "                    if 150 < mouse_pos[0] < 650 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        try:\n",
    "                            if demo is None and method is None:\n",
    "                                if i == 3:  # Visualize only\n",
    "                                    result = \"Opening visualization... Check browser!\"\n",
    "                                    pygame.display.flip()\n",
    "                                    route_path = os.path.join(os.getcwd(), \"route\")\n",
    "                                    cmd = f'cd \"{route_path}\" ; python visualization.py ; Read-Host \"Press Enter to close\"'\n",
    "                                    subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                                   creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                elif i == 4:  # Grader\n",
    "                                    result = \"Opening terminal to run grader tests...\"\n",
    "                                    pygame.display.flip()\n",
    "                                    route_path = os.path.join(os.getcwd(), \"route\")\n",
    "                                    cmd = f'cd \"{route_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                    subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                                   creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                    result = \"Grader tests running! Check terminal window\"\n",
    "                                else:  # Back to main menu\n",
    "                                    running = False\n",
    "                                break\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            route_path = os.path.join(os.getcwd(), \"route\")\n",
    "                            cmd = f'cd \"{route_path}\" ; python submission.py --demo {demo} --method {method} ; python visualization.py ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Check terminal & browser\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Car Simulation Menu - Opens car tracking in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def car_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        (\"Stationary Car\", \"-a -p -d -k 1 -i exactInference\"),\n",
    "        (\"Moving Car\", \"-a -d -k 1 -i exactInference\"),\n",
    "        (\"Multiple Cars (3)\", \"-a -d -k 3 -i exactInference\"),\n",
    "        (\"Lombard Street (3 cars)\", \"-a -d -k 3 -i exactInference -l lombard\"),\n",
    "        (\"Particle Filter (1000)\", \"-a -d -k 1 -i particleFilter -p 1000\"),\n",
    "        (\"Sensor Deception (3 cars)\", \"-a -p -d -k 3 -i exactInferenceWithSensorDeception\"),\n",
    "        (\"Run Full Grader Tests\", \"grader\"),\n",
    "        (\"Back to Main Menu\", None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "    scroll_offset = 0\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Car Tracking\", font, WHITE, 250, 50)\n",
    "\n",
    "        # Display options with scrolling support\n",
    "        visible_options = 8  # Show 8 options at once\n",
    "        for i in range(scroll_offset, min(scroll_offset + visible_options, len(options))):\n",
    "            display_idx = i - scroll_offset\n",
    "            name, _ = options[i]\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 100, 120 + display_idx * 55, 600, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            # Display result at bottom\n",
    "            draw_text(screen, result[:70], small_font, GREEN, 50, 560)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i in range(scroll_offset, min(scroll_offset + visible_options, len(options))):\n",
    "                    display_idx = i - scroll_offset\n",
    "                    name, flags = options[i]\n",
    "                    if 100 < mouse_pos[0] < 700 and 120 + display_idx * 55 < mouse_pos[1] < 170 + display_idx * 55:\n",
    "                        try:\n",
    "                            if flags is None:\n",
    "                                running = False\n",
    "                                break\n",
    "                            if flags == \"grader\":\n",
    "                                result = \"Opening terminal to run grader tests...\"\n",
    "                                pygame.display.flip()\n",
    "                                car_path = os.path.join(os.getcwd(), \"car\")\n",
    "                                cmd = f'cd \"{car_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                result = \"Grader tests running! Check terminal window\"\n",
    "                                break\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            car_path = os.path.join(os.getcwd(), \"car\")\n",
    "                            cmd = f'cd \"{car_path}\" ; python drive.py {flags} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Watch the car window\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# Start the application\n",
    "if __name__ == \"__main__\":\n",
    "    main_menu(screen)\n",
    "    pygame.quit()\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d94a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robotics\\miniconda3\\envs\\cs221\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robotics\\miniconda3\\envs\\cs221\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Import assignment modules\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'foundations'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'sentiment'))\n",
    "import foundations.submission as foundations\n",
    "import sentiment.submission as sentiment\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "BLUE = (30, 144, 255)\n",
    "GREEN = (50, 205, 50)\n",
    "RED = (255, 69, 0)\n",
    "\n",
    "font = pygame.font.Font(None, 48)\n",
    "small_font = pygame.font.Font(None, 28)\n",
    "\n",
    "# Screen setup\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Stanford CS221 Assignments Tester\")\n",
    "\n",
    "# Helper Functions\n",
    "def draw_text(surface, text, font, color, x, y):\n",
    "    text_obj = font.render(text, True, color)\n",
    "    surface.blit(text_obj, (x, y))\n",
    "\n",
    "def draw_button(surface, text, x, y, w, h, inactive_color, active_color, mouse_pos):\n",
    "    if x < mouse_pos[0] < x + w and y < mouse_pos[1] < y + h:\n",
    "        pygame.draw.rect(surface, active_color, (x, y, w, h))\n",
    "    else:\n",
    "        pygame.draw.rect(surface, inactive_color, (x, y, w, h))\n",
    "    draw_text(surface, text, small_font, WHITE, x + 10, y + 10)\n",
    "\n",
    "# -----------------------------\n",
    "# Main Menu\n",
    "# -----------------------------\n",
    "def main_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        \"1. Foundations\",\n",
    "        \"2. Sentiment Analysis\",\n",
    "        \"3. Pacman\",\n",
    "        \"4. Mountaincar\",\n",
    "        \"5. Scheduling\",\n",
    "        \"6. Route Finding\",\n",
    "        \"7. Car Tracking\",\n",
    "        \"8. Quit\"\n",
    "    ]\n",
    "    \n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"CS221 Assignments\", font, WHITE, 200, 50)\n",
    "        \n",
    "        for i, option in enumerate(options):\n",
    "            draw_button(screen, option, 250, 150 + i * 60, 300, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if 250 < mouse_pos[0] < 550:\n",
    "                    if 150 < mouse_pos[1] < 200:\n",
    "                        foundations_menu(screen)\n",
    "                    elif 210 < mouse_pos[1] < 260:\n",
    "                        sentiment_menu(screen)\n",
    "                    elif 270 < mouse_pos[1] < 320:\n",
    "                        pacman_menu(screen)\n",
    "                    elif 330 < mouse_pos[1] < 380:\n",
    "                        mountaincar_menu(screen)\n",
    "                    elif 390 < mouse_pos[1] < 440:\n",
    "                        scheduling_menu(screen)\n",
    "                    elif 450 < mouse_pos[1] < 500:\n",
    "                        route_menu(screen)\n",
    "                    elif 510 < mouse_pos[1] < 560:\n",
    "                        car_menu(screen)\n",
    "                    elif 570 < mouse_pos[1] < 620:\n",
    "                        running = False\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Foundations Menu - Interactive GUI with text input\n",
    "# -----------------------------\n",
    "def foundations_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        \"1. Find First Word\",\n",
    "        \"2. Euclidean Distance\",\n",
    "        \"3. Mutate Sentences\",\n",
    "        \"4. Dot Product\",\n",
    "        \"5. Increment Vector\",\n",
    "        \"6. Find Duplicate Words\",\n",
    "        \"7. Run Full Grader Tests\",\n",
    "        \"8. Back\"\n",
    "    ]\n",
    "    \n",
    "    selected_test = None\n",
    "    input_text = \"\"\n",
    "    input_text2 = \"\"\n",
    "    result_lines = []\n",
    "    input_active = False\n",
    "    input_box = pygame.Rect(100, 240, 600, 40)\n",
    "    input_box2 = pygame.Rect(100, 320, 600, 40)\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        \n",
    "        if selected_test is None:\n",
    "            # Main menu\n",
    "            draw_text(screen, \"Foundations Tests\", font, WHITE, 220, 30)\n",
    "            for i, option in enumerate(options):\n",
    "                draw_button(screen, option, 250, 100 + i * 60, 300, 50, BLUE, GREEN, mouse_pos)\n",
    "        else:\n",
    "            # Input screen\n",
    "            draw_text(screen, selected_test, small_font, WHITE, 50, 30)\n",
    "            \n",
    "            # Instructions based on test type\n",
    "            if \"First Word\" in selected_test:\n",
    "                draw_text(screen, \"Enter text (words separated by spaces):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            elif \"Euclidean\" in selected_test:\n",
    "                draw_text(screen, \"Point 1 (x,y): e.g., 0,0\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"Point 2 (x,y): e.g., 3,4\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            elif \"Mutate\" in selected_test:\n",
    "                draw_text(screen, \"Enter sentence (e.g., 'a b c'):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            elif \"Dot Product\" in selected_test:\n",
    "                draw_text(screen, \"Vector 1: e.g., a:1,b:2\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"Vector 2: e.g., b:3,c:4\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            elif \"Increment\" in selected_test:\n",
    "                draw_text(screen, \"V1 and Scale: e.g., a:1,b:2 2\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"V2: e.g., b:3,c:4\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            elif \"Duplicate\" in selected_test:\n",
    "                draw_text(screen, \"Enter text:\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            \n",
    "            # Compute and Back buttons\n",
    "            draw_button(screen, \"Compute\", 200, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            draw_button(screen, \"Back\", 450, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            \n",
    "            # Display results\n",
    "            y_offset = 450\n",
    "            for i, line in enumerate(result_lines[-6:]):\n",
    "                draw_text(screen, line[:80], small_font, GREEN, 50, y_offset + i * 25)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            \n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if selected_test is None:\n",
    "                    # Menu selection\n",
    "                    for i in range(8):\n",
    "                        if 250 < mouse_pos[0] < 550 and 100 + i * 60 < mouse_pos[1] < 150 + i * 60:\n",
    "                            if i == 6:  # Run grader\n",
    "                                result_lines = [\"Opening terminal to run grader tests...\"]\n",
    "                                pygame.display.flip()\n",
    "                                foundations_path = os.path.join(os.getcwd(), \"foundations\")\n",
    "                                cmd = f'cd \"{foundations_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            elif i == 7:\n",
    "                                running = False\n",
    "                            else:\n",
    "                                selected_test = options[i]\n",
    "                                input_text = \"\"\n",
    "                                input_text2 = \"\"\n",
    "                                result_lines = []\n",
    "                            break\n",
    "                else:\n",
    "                    # Check if clicking in input boxes\n",
    "                    input_active = input_box.collidepoint(event.pos) or input_box2.collidepoint(event.pos)\n",
    "                    \n",
    "                    # Compute button\n",
    "                    if 200 < mouse_pos[0] < 350 and 380 < mouse_pos[1] < 430:\n",
    "                        try:\n",
    "                            from collections import defaultdict\n",
    "                            result_lines = []\n",
    "                            \n",
    "                            if \"First Word\" in selected_test:\n",
    "                                result = foundations.find_alphabetically_first_word(input_text)\n",
    "                                result_lines.append(f\"Input: {input_text}\")\n",
    "                                result_lines.append(f\"First word alphabetically: {result}\")\n",
    "                            elif \"Euclidean\" in selected_test:\n",
    "                                p1 = tuple(map(float, input_text.split(',')))\n",
    "                                p2 = tuple(map(float, input_text2.split(',')))\n",
    "                                result = foundations.euclidean_distance(p1, p2)\n",
    "                                result_lines.append(f\"Point 1: {p1}, Point 2: {p2}\")\n",
    "                                result_lines.append(f\"Euclidean Distance: {result}\")\n",
    "                            \n",
    "                            elif \"Mutate\" in selected_test:\n",
    "                                result = foundations.mutate_sentences(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}'\")\n",
    "                                result_lines.append(f\"Found {len(result)} mutations:\")\n",
    "                                for r in result[:5]:\n",
    "                                    result_lines.append(f\"  {r}\")\n",
    "                                if len(result) > 5:\n",
    "                                    result_lines.append(f\"  ... and {len(result)-5} more\")\n",
    "                            \n",
    "                            elif \"Dot Product\" in selected_test:\n",
    "                                v1 = defaultdict(float)\n",
    "                                for pair in input_text.split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v1[k.strip()] = float(v.strip())\n",
    "                                v2 = defaultdict(float)\n",
    "                                for pair in input_text2.split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v2[k.strip()] = float(v.strip())\n",
    "                                result = foundations.sparse_vector_dot_product(v1, v2)\n",
    "                                result_lines.append(f\"V1: {dict(v1)}\")\n",
    "                                result_lines.append(f\"V2: {dict(v2)}\")\n",
    "                                result_lines.append(f\"Dot Product: {result}\")\n",
    "                            \n",
    "                            elif \"Increment\" in selected_test:\n",
    "                                parts = input_text.split()\n",
    "                                scale = float(parts[-1])\n",
    "                                v1 = defaultdict(float)\n",
    "                                for pair in ' '.join(parts[:-1]).split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v1[k.strip()] = float(v.strip())\n",
    "                                v2 = defaultdict(float)\n",
    "                                for pair in input_text2.split(','):\n",
    "                                    k, v = pair.split(':')\n",
    "                                    v2[k.strip()] = float(v.strip())\n",
    "                                result_lines.append(f\"Before: {dict(v1)}\")\n",
    "                                foundations.increment_sparse_vector(v1, scale, v2)\n",
    "                                result_lines.append(f\"After += {scale}*V2: {dict(v1)}\")\n",
    "                            \n",
    "                            elif \"Duplicate\" in selected_test:\n",
    "                                result = foundations.find_nonsingleton_words(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}'\")\n",
    "                                result_lines.append(f\"Words appearing >1: {result}\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            result_lines = [f\"Error: {str(e)}\", \"Check your input format\"]\n",
    "                    \n",
    "                    # Back button\n",
    "                    if 450 < mouse_pos[0] < 600 and 380 < mouse_pos[1] < 430:\n",
    "                        selected_test = None\n",
    "                        input_text = \"\"\n",
    "                        input_text2 = \"\"\n",
    "                        result_lines = []\n",
    "            \n",
    "            if event.type == pygame.KEYDOWN and selected_test is not None:\n",
    "                if input_box.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text = input_text[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text += event.unicode\n",
    "                elif input_box2.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text2 = input_text2[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text2 += event.unicode\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Sentiment Analysis Menu - Interactive GUI with text input\n",
    "# -----------------------------\n",
    "def sentiment_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        \"1. Extract Word Features\",\n",
    "        \"2. Character N-Grams (n=3)\",\n",
    "        \"3. Test K-Means Clustering\",\n",
    "        \"4. Run Full Grader Tests\",\n",
    "        \"5. Back\"\n",
    "    ]\n",
    "    \n",
    "    selected_test = None\n",
    "    input_text = \"\"\n",
    "    input_text2 = \"\"\n",
    "    result_lines = []\n",
    "    input_box = pygame.Rect(100, 240, 600, 40)\n",
    "    input_box2 = pygame.Rect(100, 320, 600, 40)\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        \n",
    "        if selected_test is None:\n",
    "            # Main menu\n",
    "            draw_text(screen, \"Sentiment Analysis\", font, WHITE, 200, 30)\n",
    "            for i, option in enumerate(options):\n",
    "                draw_button(screen, option, 250, 100 + i * 60, 300, 50, BLUE, GREEN, mouse_pos)\n",
    "        else:\n",
    "            # Input screen\n",
    "            draw_text(screen, selected_test, small_font, WHITE, 50, 30)\n",
    "            \n",
    "            # Instructions based on test type\n",
    "            if \"Word Features\" in selected_test:\n",
    "                draw_text(screen, \"Enter text (e.g., 'I am what I am'):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "            elif \"N-Grams\" in selected_test:\n",
    "                draw_text(screen, \"Enter text (e.g., 'I like tacos'):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"N value (default=3):\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70] if input_text2 else \"3\", small_font, WHITE, 105, 325)\n",
    "            elif \"K-Means\" in selected_test:\n",
    "                draw_text(screen, \"K (number of clusters, e.g., 2):\", small_font, WHITE, 100, 200)\n",
    "                pygame.draw.rect(screen, WHITE, input_box, 2)\n",
    "                draw_text(screen, input_text[:70], small_font, WHITE, 105, 245)\n",
    "                draw_text(screen, \"Max epochs (e.g., 10):\", small_font, WHITE, 100, 280)\n",
    "                pygame.draw.rect(screen, WHITE, input_box2, 2)\n",
    "                draw_text(screen, input_text2[:70], small_font, WHITE, 105, 325)\n",
    "            \n",
    "            # Compute and Back buttons\n",
    "            draw_button(screen, \"Compute\", 200, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            draw_button(screen, \"Back\", 450, 380, 150, 50, BLUE, GREEN, mouse_pos)\n",
    "            \n",
    "            # Display results\n",
    "            y_offset = 450\n",
    "            for i, line in enumerate(result_lines[-6:]):\n",
    "                draw_text(screen, line[:80], small_font, GREEN, 50, y_offset + i * 25)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            \n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if selected_test is None:\n",
    "                    # Menu selection\n",
    "                    for i in range(5):\n",
    "                        if 250 < mouse_pos[0] < 550 and 100 + i * 60 < mouse_pos[1] < 150 + i * 60:\n",
    "                            if i == 3:  # Run grader\n",
    "                                result_lines = [\"Opening terminal to run grader tests...\"]\n",
    "                                pygame.display.flip()\n",
    "                                sentiment_path = os.path.join(os.getcwd(), \"sentiment\")\n",
    "                                cmd = f'cd \"{sentiment_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            elif i == 4:\n",
    "                                running = False\n",
    "                            else:\n",
    "                                selected_test = options[i]\n",
    "                                input_text = \"\"\n",
    "                                input_text2 = \"\"\n",
    "                                result_lines = []\n",
    "                            break\n",
    "                else:\n",
    "                    # Compute button\n",
    "                    if 200 < mouse_pos[0] < 350 and 380 < mouse_pos[1] < 430:\n",
    "                        try:\n",
    "                            result_lines = []\n",
    "                            \n",
    "                            if \"Word Features\" in selected_test:\n",
    "                                result = sentiment.extractWordFeatures(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}'\")\n",
    "                                result_lines.append(f\"Features: {result}\")\n",
    "                                result_lines.append(f\"Total unique words: {len(result)}\")\n",
    "                            \n",
    "                            elif \"N-Grams\" in selected_test:\n",
    "                                n = int(input_text2) if input_text2 else 3\n",
    "                                extractor = sentiment.extractCharacterFeatures(n)\n",
    "                                result = extractor(input_text)\n",
    "                                result_lines.append(f\"Input: '{input_text}' (n={n})\")\n",
    "                                result_lines.append(f\"Char n-grams: {list(result.items())[:10]}\")\n",
    "                                if len(result) > 10:\n",
    "                                    result_lines.append(f\"... and {len(result)-10} more\")\n",
    "                                result_lines.append(f\"Total {n}-grams: {len(result)}\")\n",
    "                            \n",
    "                            elif \"K-Means\" in selected_test:\n",
    "                                K = int(input_text) if input_text else 2\n",
    "                                maxEpochs = int(input_text2) if input_text2 else 10\n",
    "                                # Generate sample data\n",
    "                                examples = [{'x': i, 'y': i%2} for i in range(10)]\n",
    "                                centers, assignments, loss = sentiment.kmeans(examples, K, maxEpochs)\n",
    "                                result_lines.append(f\"K={K}, Max Epochs={maxEpochs}\")\n",
    "                                result_lines.append(f\"Final loss: {loss:.4f}\")\n",
    "                                result_lines.append(f\"Assignments: {assignments}\")\n",
    "                                result_lines.append(f\"Cluster centers: {centers[:2]}...\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            result_lines = [f\"Error: {str(e)}\", \"Check your input format\"]\n",
    "                    \n",
    "                    # Back button\n",
    "                    if 450 < mouse_pos[0] < 600 and 380 < mouse_pos[1] < 430:\n",
    "                        selected_test = None\n",
    "                        input_text = \"\"\n",
    "                        input_text2 = \"\"\n",
    "                        result_lines = []\n",
    "            \n",
    "            if event.type == pygame.KEYDOWN and selected_test is not None:\n",
    "                if input_box.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text = input_text[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text += event.unicode\n",
    "                elif input_box2.collidepoint(pygame.mouse.get_pos()):\n",
    "                    if event.key == pygame.K_BACKSPACE:\n",
    "                        input_text2 = input_text2[:-1]\n",
    "                    elif event.key != pygame.K_RETURN:\n",
    "                        input_text2 += event.unicode\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Pacman Menu - Opens game in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def pacman_menu(screen):\n",
    "    running = True\n",
    "    agents = [\n",
    "        (\"Reflex Agent\", \"ReflexAgent\", None),\n",
    "        (\"Minimax Agent\", \"MinimaxAgent\", None),\n",
    "        (\"Alpha-Beta Agent\", \"AlphaBetaAgent\", None),\n",
    "        (\"Expectimax Agent\", \"ExpectimaxAgent\", None),\n",
    "        (\"Run Full Grader Tests\", \"grader\", None),\n",
    "        (\"Back to Main Menu\", None, None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Pacman Agents\", font, WHITE, 250, 50)\n",
    "\n",
    "        for i, (name, _, _) in enumerate(agents):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 200, 120 + i * 60, 400, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 100, 450)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, agent_type, _) in enumerate(agents):\n",
    "                    if 200 < mouse_pos[0] < 600 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        try:\n",
    "                            if agent_type is None:\n",
    "                                running = False\n",
    "                                break\n",
    "                            if agent_type == \"grader\":\n",
    "                                result = \"Opening terminal to run grader tests...\"\n",
    "                                pygame.display.flip()\n",
    "                                pacman_path = os.path.join(os.getcwd(), \"pacman\")\n",
    "                                cmd = f'cd \"{pacman_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                result = \"Grader tests running! Check terminal window\"\n",
    "                                break\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            # Open in new PowerShell window\n",
    "                            pacman_path = os.path.join(os.getcwd(), \"pacman\")\n",
    "                            cmd = f'cd \"{pacman_path}\" ; python pacman.py -p {agent_type} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} is playing! Watch the game window\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Mountaincar Menu - Opens training in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def mountaincar_menu(screen):\n",
    "    running = True\n",
    "    agents = [\n",
    "        (\"Value Iteration\", \"--agent\", \"value-iteration\"),\n",
    "        (\"Tabular Q-Learning\", \"--agent\", \"tabular\"),\n",
    "        (\"Function Approximation\", \"--agent\", \"function-approximation\"),\n",
    "        (\"Constrained Q-Learning\", \"--agent\", \"constrained\"),\n",
    "        (\"Visualize: Naive Agent\", \"visualize\", \"naive\"),\n",
    "        (\"Visualize: Value Iteration\", \"visualize\", \"value-iteration\"),\n",
    "        (\"Visualize: Tabular Q-Learning\", \"visualize\", \"tabular\"),\n",
    "        (\"Visualize: Function Approx\", \"visualize\", \"function-approximation\"),\n",
    "        (\"Run Full Grader Tests\", \"grader\", None),\n",
    "        (\"Back to Main Menu\", None, None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Mountaincar Agents\", font, WHITE, 250, 50)\n",
    "\n",
    "        for i, (name, _, _) in enumerate(agents):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 200, 120 + i * 60, 400, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 100, 450)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, flag_name, flag_value) in enumerate(agents):\n",
    "                    if 200 < mouse_pos[0] < 600 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        if flag_name is None:  # Back to main menu\n",
    "                            running = False\n",
    "                            break\n",
    "                        if flag_name == \"visualize\":  # Run visualization\n",
    "                            result = f\"Starting {name} visualization...\"\n",
    "                            pygame.display.flip()\n",
    "                            mountaincar_path = os.path.join(os.getcwd(), \"mountaincar\")\n",
    "                            cmd = f'cd \"{mountaincar_path}\" ; python mountaincar.py --agent {flag_value} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Watch the visualization window\"\n",
    "                            break\n",
    "                        if flag_name == \"grader\":  # Run grader tests\n",
    "                            result = \"Opening terminal to run grader tests...\"\n",
    "                            pygame.display.flip()\n",
    "                            mountaincar_path = os.path.join(os.getcwd(), \"mountaincar\")\n",
    "                            cmd = f'cd \"{mountaincar_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = \"Grader tests running! Check terminal window\"\n",
    "                            break\n",
    "                        try:\n",
    "                            result = f\"Starting {name}... Check new terminal window!\"\n",
    "                            pygame.display.flip()\n",
    "                            # Open in new PowerShell window so training output and plots are visible\n",
    "                            mountaincar_path = os.path.join(os.getcwd(), \"mountaincar\")\n",
    "                            cmd = f'cd \"{mountaincar_path}\" ; python train.py {flag_name} {flag_value} ; Start-Sleep -Seconds 1 ; Get-ChildItem -Filter \"*.png\" | Sort-Object LastWriteTime -Descending | Select-Object -First 3 | ForEach-Object {{ Start-Process $_.FullName }} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd], \n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} is training! Plots will open automatically\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Scheduling Menu - Opens scheduling tests in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def scheduling_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        (\"Run N-Queens Test\", \"test_nqueens.py\"),\n",
    "        (\"Run Problem 2 Tests\", \"run_p2.py\"),\n",
    "        (\"Run Problem 3 Tests\", \"run_p3.py\"),\n",
    "        (\"Run Full Grader Tests\", \"grader.py\"),\n",
    "        (\"Back to Main Menu\", None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Scheduling & CSP\", font, WHITE, 220, 50)\n",
    "\n",
    "        for i, (name, _) in enumerate(options):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 200, 120 + i * 60, 400, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 100, 450)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, script) in enumerate(options):\n",
    "                    if 200 < mouse_pos[0] < 600 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        if script is None:\n",
    "                            running = False\n",
    "                            break\n",
    "                        try:\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            scheduling_path = os.path.join(os.getcwd(), \"scheduling\")\n",
    "                            cmd = f'cd \"{scheduling_path}\" ; python {script} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Watch terminal window\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Route Menu - RUNS route demos and visualization in NEW TERMINAL WINDOW with conda run\n",
    "# -----------------------------\n",
    "def route_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        (\"Shortest (UCS)\", \"shortest\", \"ucs\"),\n",
    "        (\"Shortest (A*)\", \"shortest\", \"astar\"),\n",
    "        (\"Waypoints (UCS)\", \"waypoints\", \"ucs\"),\n",
    "        (\"Visualize Last Path\", None, None),\n",
    "        (\"Run Full Grader Tests\", \"grader\", None),\n",
    "        (\"Back to Main Menu\", None, None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Route Planning\", font, WHITE, 250, 50)\n",
    "\n",
    "        for i, (name, _, _) in enumerate(options):\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 150, 120 + i * 60, 500, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            draw_text(screen, result, small_font, GREEN, 80, 500)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i, (name, demo, method) in enumerate(options):\n",
    "                    if 150 < mouse_pos[0] < 650 and 120 + i * 60 < mouse_pos[1] < 170 + i * 60:\n",
    "                        try:\n",
    "                            if demo is None and method is None:\n",
    "                                if i == 3:  # Visualize only\n",
    "                                    result = \"Opening visualization... Check browser!\"\n",
    "                                    pygame.display.flip()\n",
    "                                    route_path = os.path.join(os.getcwd(), \"route\")\n",
    "                                    cmd = f'cd \"{route_path}\" ; python visualization.py ; Read-Host \"Press Enter to close\"'\n",
    "                                    subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                                   creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                elif i == 4:  # Grader\n",
    "                                    result = \"Opening terminal to run grader tests...\"\n",
    "                                    pygame.display.flip()\n",
    "                                    route_path = os.path.join(os.getcwd(), \"route\")\n",
    "                                    cmd = f'cd \"{route_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                    subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                                   creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                    result = \"Grader tests running! Check terminal window\"\n",
    "                                else:  # Back to main menu\n",
    "                                    running = False\n",
    "                                break\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            route_path = os.path.join(os.getcwd(), \"route\")\n",
    "                            cmd = f'cd \"{route_path}\" ; python submission.py --demo {demo} --method {method} ; python visualization.py ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Check terminal & browser\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# -----------------------------\n",
    "# Car Simulation Menu - Opens car tracking in NEW TERMINAL WINDOW\n",
    "# -----------------------------\n",
    "def car_menu(screen):\n",
    "    running = True\n",
    "    options = [\n",
    "        (\"Stationary Car\", \"-a -p -d -k 1 -i exactInference\"),\n",
    "        (\"Moving Car\", \"-a -d -k 1 -i exactInference\"),\n",
    "        (\"Multiple Cars (3)\", \"-a -d -k 3 -i exactInference\"),\n",
    "        (\"Lombard Street (3 cars)\", \"-a -d -k 3 -i exactInference -l lombard\"),\n",
    "        (\"Particle Filter (1000)\", \"-a -d -k 1 -i particleFilter -p 1000\"),\n",
    "        (\"Sensor Deception (3 cars)\", \"-a -p -d -k 3 -i exactInferenceWithSensorDeception\"),\n",
    "        (\"Run Full Grader Tests\", \"grader\"),\n",
    "        (\"Back to Main Menu\", None),\n",
    "    ]\n",
    "    result = \"\"\n",
    "    scroll_offset = 0\n",
    "\n",
    "    while running:\n",
    "        screen.fill(BLACK)\n",
    "        mouse_pos = pygame.mouse.get_pos()\n",
    "        draw_text(screen, \"Car Tracking\", font, WHITE, 250, 50)\n",
    "\n",
    "        # Display options with scrolling support\n",
    "        visible_options = 8  # Show 8 options at once\n",
    "        for i in range(scroll_offset, min(scroll_offset + visible_options, len(options))):\n",
    "            display_idx = i - scroll_offset\n",
    "            name, _ = options[i]\n",
    "            draw_button(screen, f\"{i+1}. {name}\", 100, 120 + display_idx * 55, 600, 50, BLUE, GREEN, mouse_pos)\n",
    "        \n",
    "        if result:\n",
    "            # Display result at bottom\n",
    "            draw_text(screen, result[:70], small_font, GREEN, 50, 560)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                for i in range(scroll_offset, min(scroll_offset + visible_options, len(options))):\n",
    "                    display_idx = i - scroll_offset\n",
    "                    name, flags = options[i]\n",
    "                    if 100 < mouse_pos[0] < 700 and 120 + display_idx * 55 < mouse_pos[1] < 170 + display_idx * 55:\n",
    "                        try:\n",
    "                            if flags is None:\n",
    "                                running = False\n",
    "                                break\n",
    "                            if flags == \"grader\":\n",
    "                                result = \"Opening terminal to run grader tests...\"\n",
    "                                pygame.display.flip()\n",
    "                                car_path = os.path.join(os.getcwd(), \"car\")\n",
    "                                cmd = f'cd \"{car_path}\" ; python grader.py ; Read-Host \"Press Enter to close\"'\n",
    "                                subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                               creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                                result = \"Grader tests running! Check terminal window\"\n",
    "                                break\n",
    "                            result = f\"Starting {name}... Check new window!\"\n",
    "                            pygame.display.flip()\n",
    "                            car_path = os.path.join(os.getcwd(), \"car\")\n",
    "                            cmd = f'cd \"{car_path}\" ; python drive.py {flags} ; Read-Host \"Press Enter to close\"'\n",
    "                            subprocess.Popen(['powershell.exe', '-NoExit', '-Command', cmd],\n",
    "                                           creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "                            result = f\"{name} running! Watch the car window\"\n",
    "                        except Exception as e:\n",
    "                            result = f\"Error: {e}\"\n",
    "                        break\n",
    "        pygame.display.flip()\n",
    "\n",
    "# Start the application\n",
    "if __name__ == \"__main__\":\n",
    "    main_menu(screen)\n",
    "    pygame.quit()\n",
    "    sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
